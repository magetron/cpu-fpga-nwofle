\documentclass[a4paper]{report}
\usepackage{setspace}
\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{footmisc}
\usepackage{xcolor}
\usepackage{pdflscape}
\usepackage{textcomp}
\usepackage[backend=biber,style=numeric,sortcites,sorting=none]{biblatex}
\usepackage{subfig}
\addbibresource{report.bib}
\usepackage[a4paper, margin = 3cm, bottom = 2.5cm]{geometry}
\usepackage{fontspec}
\setmonofont[
  Contextuals={Alternate}
]{Fira Code}

\counterwithout{footnote}{chapter}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\proglang}{\textsf}
\newcommand{\pkg}{\textbf}
\newcommand{\code}{\texttt}
\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[-] #1}
}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{code-style}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}
\lstset{style=code-style}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}

\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}


\title{{\vspace{-14em}}
{{\Huge High-performance network anomaly detection and threat mitigation via hardware accelerated DNS spoofing and traffic filtering}} \\
{\large}}

\date{Submission date: 27 March 2021}
\author{Patrick (Daiqi) Wu\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for Patrick (Daiqi) Wu's BSc Computer Science degree at University College London (UCL). It is
substantially the result of my own work except where explicitly indicated in the text.
The report will be distributed to the internal and external examiners, but thereafter may be copied and distributed under the Creative Commons -- Attribution 4.0 International License \cite{cc-by-4.0}.}
\\
BSc Computer Science\\ \\
Supervisors: Professor Stephen Hailes, Phil Demetriou}

\begin{document}
 
\onehalfspacing
\maketitle
\begin{abstract}
Summarise your report concisely.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
Acknowledges help and support from Prof. Hailes, Phil, Monica and my parents.
\end{abstract}

\tableofcontents

\newpage
\setcounter{page}{1}

\chapter{Introduction}

\section{Problem Statement}
\label{section:introduction-problemstatement}

Nowadays, the Domain Name System (DNS) Protocol is one of the most widely and prominently used protocols built-upon the Internet Protocol \cite{RFC-1034}. As Akamai Technologies aggregated publicly available DNS data, the total amount of DNS traffic across the Internet quadrupled from $ 2 \times 10^{12}$ transactions/month (a query-reply pair) in January 2016 to $8 \times 10^{12}$ transactions/month in October 2020 \cite{DNS-Trends-and-Traffic}. Moreover, as the Internet of Things (IoT) continues to trend, an increasing number of clients, including but not limited to cars, home appliances, will be able to connect to the Internet. While every one of them would require domain name resolving service to a certain extent, DNS will play a more paramount role in the Internet infrastructure \cite{satam2015anomaly}.

The DNS lookup service is provided to Internet users from Domain Name Servers across the globe. There are two types of name servers, namely authoritative name servers and caching name servers. Authoritative name servers provide domain name translation records or delegation records designated by administrators within a given zone \cite{BCP-219}. The authoritative name servers' DNS replies to queries within their respective authoritative zones are called Authoritative Answers (AA), in which records are considered final and correct within those records' Time-To-Live (TTL) period \cite{BCP-219, RFC-1035}. In theory, the Internet will be able to operate without caching name servers. However, caching name servers vastly improve DNS lookup efficiency by caching resolved records within their TTL and further reduce DNS traffic across the Internet. Typically, caching name servers also implement a recursive resolution of domain names, which essentially resolves a query from root name servers to the queried domain's authoritative name server \cite{finch-2015}.

As demonstrated previously \cite{RFC-1034, RFC-1035}, DNS operate based on a query/reply system between clients and name servers. Since the original DNS protocol started operating on the Internet back in 1985, security concerns were not the primary design considerations, as the Internet was not accessible by the general public. Therefore, the DNS protocol's security flaw unveils attacking opportunities to be exploited under multiple circumstances\cite{antonakakis2010centralized}. Moreover, the increase in total DNS traffic makes it even harder to detect, filter and block these anomaly DNS requests \cite{kambourakis2007detecting}. 

One of the significant threat to DNS servers is the DNS flood attack, a form of Distributed Denial-of-Service (DDoS) attack. With the increase in overall traffic, the current protection methods are proven increasingly incapable of defending against these attacks. In October 2016, the Mirai botnet consisting of more than $100,000$ infected IoT devices launched a DNS flood attack against DYN DNS servers, resulting in a major service outage in DYN's clients' websites (GitHub, Netflix, Amazon, etc.) \cite{bisson-2016}. Yet, the typical solution of adding multiple software filters and checks on incoming queries in front of DNS name servers throttles the overall network traffic, reduces per-server throughput and causes the DNS reply latencies to be much higher than usual from a typical client's perspective\cite{Mahjabin-2020}.
 
As DNS plays a fundamental role in all communications across the network (excluding static-IP based communications), DNS traffic filtering is usually done conservatively, even in highly-secured enterprise networks. For the attacker, however, this makes DNS an excellent candidate to establish communication tunnels for data exfiltration from these secured networks \cite{nadler-201936}. In recent years, an increasing number of bad actors have exploited DNS for malicious purposes and have caused several notable incidents \cite{das-8260721}. A blog published by Grunzweig et al.\cite{grunzweig_scott_lee_2018} from Palo Alto Networks has demonstrated malware that uses DNS transactions as tunnels to communicate with the Command and Control (C\&C) server. Another incident shown in the study by Singh et al.\cite{singh-2016} describes malware that uses DNS query domain names to ex-filtrate secured data. Since DNS transactions are not intended for data transfer purposes, it is usually ignored by most network security tools and firewalls. Also, the vast volume of DNS traffic (over one-third of a typical enterprise network flows are DNS \cite{das-8260721}) poses significant challenges for Layer-7 firewalls to scrutinise all of them within the network. Furthermore, the standard approach of simply restricting public network access for the secured network would be insufficient against this attack, as data exfiltration is still possible via forwarded DNS queries \cite{bromberger2011dns}.

Given the DNS is such a critical service that it is unlikely to be replaced within the foreseeable future, the project intends to tackle the problem of detecting and mitigating DNS data exfiltration and DNS flood attacks with minimal latency overhead under high-throughput circumstances.

\section{Project Aims and Goals}

The project aims to develop a detection and mitigation solution against DNS exfiltration and flood attacks without introducing substantial latency or performance impact to the network.

Leveraging the unparalleled network processing capability of computing hardware, namely Application-Specific Integrated Circuit (ASIC) and Field Programmable Gate Arrays (FPGA), the solution will bring an extra level of security with minimal overhead during the DNS transaction process. Furthermore, we aim to provide an easily adaptable computing hardware solution that can be side-loaded / connected to a network without any significant firewall or overall topology changes.

The final delivered product shall include a piece of computing hardware that detects malicious DNS transactions in real-time and mitigate/stop the attack from executing properly. DNS data exfiltration can be prevented via hardware-accelerated DNS filtering and spoofing, while DNS flood can be mitigated via hardware traffic blocking. The delivered solution shall be versatile and instantly re-configurable via a software control system to defend against changing malicious sources. Finally, the whole solution shall be extensible, maintainable and scalable on different hardware platforms for various budgets, traffic loads, and processing power needs in different types of networked systems.

\section{Project Approach}

The project was developed from scratch for maximum control over the hardware logic and performance without using any pre-developed Intellectual Property (IP) cores from Xilinx.

Conversely, the traditional bottom-up/top-down approach in software engineering would not be suitable as hardware integration is notoriously tricky. The final integration behaviour could be non-deterministic and undebuggable, let alone integrating several components simultaneously. During the project, there are many cases that we experience unexpected and unreasonable behaviour in the hardware, so we had to revert to the last test-passing version and re-examine our development approach towards the component.

Hence, we took an incremental approach when developing the project. It is considered a better-suited approach for a rather complex hardware-software coordination project. Each component has its unique challenges and design, and it is in our best interest to make sure it passes software simulation, hardware testing, stress testing, integration testing in hardware and eventually software control system testing before moving on to the subsequent development process.

\section{Tools and Utilities}

\subsection{Hardware}
\label{section:introduction-tools-hardware}

We choose Xilinx FPGA instead of ASIC as our hardware development target platform due to budget constraint and our concerns for future-proof re-programmability.

We started the development with:
\begin{itemize}
    \item Board: \code{Xilinx Spartan-3E FPGA Starter Kit Board} \cite{xilinx-documentation-2011}
    \item FPGA Core: \code{Xilinx Spartan-3E FPGA} \cite{xilinx-documentation-2011-core}
    \item Hardware IDE: \code{Xilinx ISE Design Suite 14.7} \cite{xilinx-documentation-ise}
\end{itemize}

During the development process, the design is constrained by the Slice Look-Up Table (LUT) limit in the Spartan-3E FPGA core. 

Hence, we moved new development platform:
\begin{itemize}
    \item Board: \code{Digilent Arty A7-35T development board} \cite{digilent-arty}
    \item FPGA Core: \code{Xilinx Artix-7 FPGA} \cite{xilinx-documentation-artix}
    \item Hardware IDE: \code{Xilinx Vivado Design Suite 2020.2} \cite{xilinx-documentation-vivado}
\end{itemize}

We choose \proglang{VHDL-2008} \cite{ieee-vhdl} as our main Hardware-Description Language (HDL) for the design. Also, \proglang{Verilog} \cite{ieee-verilog} is used for timing simulation of synthesised net-list designs. \code{Xilinx Vivado xsim} \cite{xilinx-documentation-vivado} is used for hardware design simulation. The \code{Easics CRC Generation Tool} is used for generating \proglang{VHDL} \code{Ethernet-II CRC32} processing component \cite{easics-crc}.

\subsection{Software}
\label{section:introduction-tools-software}

As of the software, the control system and various tool-sets are written in \proglang{C11} \cite{iso-c} and \proglang{C++17} \cite{iso-cc}, using \code{libedit} \cite{thrysoee-2004} for Command Line Interface (CLI) and the raw socket programming interfaces (APIs) provided with the \code{Linux kernel 5.10.23} \cite{kroah-hartman-2021} for communication with the FPGA.

Also, \code{SipHash} \cite{aumasson-bernstein-2012} is used in both hardware and software for packet authentication and admin verification purposes.

\subsection{Debugging}
\label{section:introduction-tools-debugging}

When debugging on and above the data link layer, we used \code{Wireshark 3.4.4} \cite{wireshark-2021}, \code{tcpdump 4.99.0}
 \cite{tcpdump-2020}, \code{ethtool 5.10} \cite{kroah-hartman-2021} and the Linux \code{ip} command. \code{Ethernet-CRC32-Checker} \cite{jwbensley-2020} is used for Ethernet CRC debugging in FPGA on the PHY level.

\section{Report Structure}
The report will be laid out as follows:

\begin{enumerate}[leftmargin=*, label=Chapter \arabic* - ]
\setcounter{enumi}{1}
\item Background information about DNS and its vulnerabilities in detail, FPGA and its usages in network processing, and a survey of the related work and how this project will contribute to the field
\item Design of the hardware and software components of the project, and sample topologies of a network using the project against DNS exfiltration and flood attacks
\item Implementation of the hardware and software components, ranging from behavioural simulation, hardware timing simulation, hardware implementation, hardware debugging to software implementation
\item Validation of the solution correctness and evaluation of the overall system performance
\item Conclusion of the project progress and future work directions if this project were to be further developed and deployed commercially
\end{enumerate}

\chapter{Background and Related Work}

In this chapter, we will be introducing the DNS, including how it works, some security improvements to DNS, and the major security issues of DNS this project is targetting. Also, we will be covering how computing hardware, most notably FPGAs, will be applicable in computer networks and the reason why it is instrumental in solving the aforementioned DNS issues. Besides, we will be briefing Sip Hash, a hash MAC algorithm used in the project. Lastly, we will be presenting related work in DNS anomaly detection and threat mitigation and relevant FPGA/hardware work in this area.

\section{Background}

\subsection{Domain Name System (DNS)}
DNS is a distributed, hierarchical naming system for computing nodes or resources within a network \cite{RFC-1034}. It provides the backbone service of associating each entity's information with the domain names assigned to them \cite{RFC-1034, RFC-1035}. Notably, on the Internet, DNS service is used when translating more readily memorised domain names to numerical IP addresses needed for locating servicing nodes in the network with the underlying IP Protocol \cite{RFC-1034, RFC-791}.

A DNS resolution example will be provided below to illustrate how DNS work in a typical network environment. When the IP address of a domain name is requested by a person or a service (i.e. \url{www.apple.com}), the local cache will be the first lookup point of the query. If no match is found, the DNS query will be forwarded to a local DNS resolver, then to a public DNS resolver (i.e. Cloudflare's public DNS server at IPv4 \url{1.1.1.1}). If the query remains unresolved, it will then get forwarded to the root name servers to obtain authoritative name servers' address of \url{.com}. Then, querying \url{.com} authoritative name servers will provide further information of \url{apple.com}'s authoritative name servers. The resolution progresses recursively until the domain name gets resolved and the final IP address is obtained and replied back to the inquirer.

Over the years, there are more secured replacement of DNS proposed, most notably the Domain Name System Security Extensions (DNSSEC), originally proposed in 1997\cite{RFC-2535, RFC-4033}. Conversely, a study from Chung et al. \cite{chung-2017} showed that only 3 of the top 20 registrars support DNSSEC when the registrar is the DNS operator, and only 1 of the top 20 enables DNSSEC by default in their more expensive domains. On second-level domains, roughly 1\% of \url{.com}, \url{.net} and \url{.org} domains deploy DNSSEC \cite{chung-2017-1}. RFC-3833 \cite{RFC-3833} outlines several major weakness of DNSSEC, including but not limited to the complexity of implementation, significant increase and packet sizes and inability to defend against certain security issues (DNS exfiltration, DNS flood). Moreover, this project utilises spoofed DNS records against malicious DNS tunnelling. Implementing DNSSEC would deem this method invalid across the network.

\subsection{DNS Data Exfiltration}

Arbitrary data exchange was never the design purpose of the DNS protocol \cite{nadler-201936}. DNS query messages are typically short and compact, while their corresponding DNS response would not be guaranteed to arrive in the same order as they were queried \cite{RFC-1034}. Nevertheless, it is still achievable to exchange data reliably or secretly over the DNS protocol.
We will illustrate two examples of malware extracting confidential data from a host or executes commands from the attacking C\&C. 

In Figure \ref{fig:dns-data-exfil-cycle}, the malware detects a secret file from its host; hence it constructs a DNS query with the secret phrase, or more likely a \code{base64} encoded phrase to circumvent the firewall, as the Third-Level Domain (3LD) in the DNS query. As demonstrated in Section \ref{section:introduction-problemstatement}, the local DNS resolver would be unable to find it in the cache, and hence this query will eventually reach \url{malicious.com}'s authoritative name server. Since the attacker has complete control over \url{malicious.com}, it would then be able to retrieve the secret phrase from a highly secured host.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{imgs/dns-data-exfil-out.png}
  \caption{Malware extracting password from host through DNS}
  \label{fig:dns-data-exfil-out}
\end{figure}

Similarly, Figure \ref{fig:dns-data-exfil-cycle} demonstrates an example where the malware queries C\&C for commands to archive remote control on the compromised host. The DNS query remains similar to the example above in Figure \ref{fig:dns-data-exfil-out}; the difference lies in the DNS reply from the \url{malicious.com} name server. The attacker can respond with a Canonical Name (CNAME) Record indicating the instruction, usually with a very short TTL, so as to avoid this instruction getting cached. This DNS query getting resolved on the compromised host will instruct the malware to act as ordered, in this particular figure, to read files and folders in \url{~/Documents}.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{imgs/dns-data-exfil-cycle.png}
  \caption{Malware executing commands from remote C\&C}
  \label{fig:dns-data-exfil-cycle}
\end{figure}

Malware exploiting DNS for data exchanges can be divided into two major approaches: high-throughput tunnelling and low-throughput tunnelling.

As of high-throughput tunnelling, the malware would establish a bi-directional communication channel on top of the DNS via emulating a reliable and packet ordered network session between the compromised server and its C\&C. For example, the malware can enforce a Transmission Control Protocol (TCP) on top of DNS or send constant ping messages throughout communication to understand the replies' order. However, applying these methodologies would significantly increase the total amount of DNS traffic and each DNS packet's length while vastly decreasing the meantime between DNS transactions \cite{nadler-2017, nadler-201936}. As seen in Figure \ref{fig:dns-high-throughput-tunnelling}, the query intervals are less than 1 second due to the regular session keep-alive messages, the response towards the same queried domain \url{paaqfiiq.iodine.exfiltration.party} varies notably to signal a two-way communication tunnel, and some query length exceeds 200 bytes. These unusual characteristics compared to normal DNS traffic implies the existence of data exchange and exfiltration on top of DNS \cite{nadler-2017}.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{imgs/data-exfil-high-throughput-pcap.jpg}
  \caption{Sample \url{.pcap} file captured when running high-throughput DNS tunnelling}
  \label{fig:dns-high-throughput-tunnelling}
\end{figure}

Compared to high-throughput tunnelling, low-throughput tunnelling is generally harder to detect and remains overlooked within the cyber-security community \cite{nadler-2017, nadler-201936, ahmed-2020}.
To circumvent anomaly detection methods and firewalls, attackers would typically avoid using TCP tunnels or sending constant ping messages. Alternatively, the malware would wake up periodically, or triggered by specific activities, to deliver poll messages and summary of machine activities to the C\&C \cite{nadler-2018}. For example, an infected Point-of-Sale (POS) terminal would be triggered upon a swipe and send transaction and card details to the attacker's C\&C without waiting for a response.

Detecting DNS tunnelling has been widely researched over the last decade, including detecting covert tunnelling \cite{gilbert-2009, davis-2016}, and high-throughput DNS tunnelling detection \cite{wang-2016, buczak-2016, cambiaso-2016, engelstad-2017, homem-2017, kara-2014, sheridan-2015}. However, most publicly available detection solutions focus on high-throughput tunnelling due to its distinguishable network behaviour. These detection solutions rely heavily on the volume and variety of requests DNS tunnelling tools generate \cite{nadler-2017, nadler-2018}. One of the most popular solutions is DNS traffic rate control, offered by most security vendors \cite{homem-2017, nadler-2017}. More sophisticated solutions include supervised learning models on tunnelling DNS traffic, and non-tunnelling DNS traffic \cite{buczak-2016}, and anomaly detection models that would trigger upon notable change over DNS traffic as a whole \cite{davis-2016, sheridan-2015}.

\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|l|l}
Year & Malware       & Targets    & Malicious Domain Name   \\ \hline
2017 & DNS Messenger & Enterprise & \url{cspg.pw, algew.me}                          \\
2016 & MULTIGRAIN    & POS        & \url{dojfgj.com}                                 \\
2015 & BernhardPOS   & POS        & \url{29a.de}                                     \\
2015 & JAKU          & Botnet     & \url{LS4.com}                                    \\
2015 & Wekby         & Enterprise & \url{ns1.logitech-usa.com}                       \\
2014 & FrameworkPOS  & POS        & \url{a23-33-37-54-deploy.akamaitechnologies.com} \\
2014 & PlugX         & RDP/RAT    & \url{ns4.msftncsl.com}                           \\
2011 & FeederBot     & Botnet     & \url{images.moviedyear.net}                      \\
2011 & Morto         & RDP/RAT    & \url{ms.jifr.co.cc., ms.jifr.net.}      
\end{tabular}
}
\caption{A survery of malware from 2011-2018 exploiting DNS for low-throughput data exfiltration and exchange \cite{nadler-201936}}
\label{table:survey-malware-low-throughput-dns}
\end{table}

Although the topic of high-throughput DNS tunnelling has been studied thoroughly, little progress has been made on low-throughput DNS data exfiltration/exchange \cite{nadler-201936, steadman-2018, ahmed-2020}. A recent survey conducted by Nadler et al. \cite{nadler-201936} shown in Table \ref{table:survey-malware-low-throughput-dns} listed a number of malware exploiting DNS for low-throughput data exfiltration/exchange exposed during 2011 to 2018. These malware targets vary from enterprise networks, botnets, POS terminals to Remote Desktop Protocol (RDP) / Remote Access Trojan (RAT). Meanwhile, the set of internet domains used by these malware includes concise ones (i.e. \url{29a.de}) to maximise the data exchange rate under the 255-byte query limit, and misleading ones (i.e. \url{ns1.logitech-usa.com}) imitating well-known brands to better hide under the human inspection of DNS traffic \cite{nadler-201936}. Detecting and blocking this type of malicious DNS traffic is much more difficult given its minimal proportion across the overall DNS traffic within an extensive network. Defending against these malware would require Deep Packet Inspection (DPI) from a Layer-7 firewall to at least parse through all DNS packets. This would induce extra latency to DNS transactions and slows down performance-critical services within the network. 

Hence, in this project, we will be targetting at building a solution that detects and blocks upon request both types of DNS data exfiltration/exchange methods reliably without introducing any significant latency to the network.

\subsection{DNS Flood}
\label{section:background-dnsflood}

DNS flood attack is a type of DDoS attack, as shown in Figure \ref{fig:dns-flood}, where the junk incoming traffic floods the victim's server, making it unable to provide normal service to legitimate users for a prolonged period \cite{cloudflare-dns-flood}. Given DNS is the backbone service of translating domain names to IPv4 addresses, successfully attacking DNS name servers can make a significant proportion of the Internet unusable for most people. Furthermore, in recent years, there has been a substantial rise in IoT devices, a considerable amount of which are poorly secured \cite{mahjabin-2019}. In 2016, a botnet malware, Mirai\cite{bisson-2016}, utilises the high-bandwidth connections of IP cameras, Network Video Recorder (NVR) boxes and many other IoT devices to directly overwhelm name servers owned by DYN, a major DNS service provider. The peak flood bandwidth reached more than $12$ Gigabits per second, with more than $1$ million packets on a single DNS name server, resulting in significant take-downs of multiple popular websites, including GitHub, Netflix, Amazon, etc. \cite{bisson-2016, akamai-dns-flood}.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{imgs/dns-flood.png}
  \caption{Botnet commencing DNS flood attack targetting a DNS name server}
  \label{fig:dns-flood}
\end{figure}

DNS flood attacks are designed to circumvent DNS resolvers' caching and continuously enable DNS queries to go to certain authoritative name servers. It is done via a technique named Pseudo-Random Subdomain (PRSD) \cite{akamai-dns-flood}. The botnet attacker targetting a domain would generate a great number of non-existent prefixed subdomain requests, causing each bots corresponding DNS resolver to continuously query the authoritative name server with a \code{NXDOMAIN} reply. In the cases where bots are conducting DNS flood are making requests to name servers directly, blocking such clients and imposing a request rate limit could be straight forward. However, it is more likely that these malicious DNS queries are coming from upstream ISP DNS resolvers or public resolves, which would not cache \code{NXDOMAIN} reply for saving resources on these resolvers \cite{akamai-dns-flood}. This complicates the DNS flood mitigation process as rate-limiting or blocking these public / ISP DNS resolvers risk throttling legitimate queries from users, which happens to share the same resolvers.

As a Cloudflare paper \cite{cloudflare-dns-flood} points out, DNS flood attack represents a change from the traditional DNS amplification based attack methods. DNS flood is an application attack (Layer 7) rather than a volumetric attack (Layer 3). There is no spoofing required for DNS flood, nor does the request comes directly from the attacking source, but upstream DNS resolvers \cite{akamai-dns-flood, cloudflare-dns-flood}. Realistically, DNS flood would not be gone until most compromised IoT devices can be secured by updating firmware or replacing easy-to-guess passwords. Therefore, besides building a large and distributed DNS name server group, we would attempt to tackle this problem via building a performant packet processing solution to absorb and block the attack traffic in real-time.

\subsection{Field Programmable Gate Arrays (FPGA)}
\label{section:background-fpga}

FPGA is a type of integrated circuit designed to be customised by a designer after its fabrication, hence "programmable on the field". As shown in Figure \ref{fig:fpga-arch}, it contains a matrix of Configurable Logic Blocks (CLBs) and higher levels of configurable interconnect, which activates and links the blocks in the manner the developer specifies \cite{xilinx-fpga}. There are two types of FPGAs, one-time programmable (OTP) ones and re-programmable ones \cite{xilinx-fpga}. Since OTP FPGAs are rare and outdated, we will mainly discuss Static Random Access Memory (SRAM) based re-programmable FPGAs.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.3\textwidth]{docs/report/imgs/fpga-arch.png}
  \caption{FPGA Architecture \cite{xilinx-fpga}}
  \label{fig:fpga-arch}
\end{figure}

Software network middlewares including firewalls \code{netfilter}/\code{iptables} \cite{netfilter-iptables}, \code{pf} \cite{pf}, \code{ipfw} \cite{ipfw} has been widely used in industrial and enterprise networks. As the network bandwidth grows, these software-based network middlewares are heavily limited by the parallelism and processing power of a single CPU. They can no longer meet the line rate packet processing requirements for large networks with high link speeds, typically $40$ Gbps and beyond \cite{fiessler-2016}. Therefore, we've seen an increasing number of packet processing systems built upon special purpose hardware, such as network processors (NPUs) \cite {qi-2007, duo-2006}, FPGAs\cite{hager-2014, fong-2012, jiang-2009, jiang-2009-large} and even ASICs \cite{bosshart-2013}. These hardware-based solutions provide an abundant amount of parallelism and can scale up almost linearly with minimal effort. Nowadays, the highest grade 7nm ASIC packet processors can archive an overall network throughput of more than $25.6$ Tbps \cite{tomahawk-2021}, with a single packet latency lower than $400$ ps, orders of magnitude faster than any of the software-based packet processors.

There are various types of trade-offs among different implementations of network processors with regards to development time and cost, as well as flexibility and long-term adaptability. Among all three, ASIC-based Network Interface Cards (NICs) have the highest performance and power efficiency but suffers from a long development cycle, high development cost, and minimal flexibility after fabrication \cite{amara-2006, deierling-2018}. Oppositely, CPU-based or System-on-Chip (SoC) NICs are easier to program and can be deployed for highly complex use cases, as CPU provides general computability and the highest flexibility among all three \cite{deierling-2018}. FPGA-based NICs strikes a balance between ASIC and SoC by providing "field-programmable" hardware. In other words, FPGA would have performance similar to that of ASIC since they are both integrated circuit for application-specific purposes while maintaining the flexibility of being adapting and upgrading after fabrication. Due to circuit limitations, the performance, energy efficiency and achievable timing frequency of FPGAs would be inferior to that of ASICs. On the other hand, FPGAs would be much more complex to program than SoC due to hardware timing constraints and electrical uncertainties, and deployed FPGA algorithm would ultimately be limited by the number of logic units on the FPGA chip \cite{deierling-2018}.

\subsection{SipHash}
\label{section:background-siphash}

SipHash is an Add-Rotate-Xor (ARX) family of pseudo-random hash functions created in 2012 \cite{aumasson-bernstein-2012}, aimed at mitigating various "hash-flooding" DDoS attacks in late 2011 \cite{lennon-2011}. SipHash has typically used as a Message Authentication Code (MAC) calculation method to guarantee the sender's authority and message's integrity. 

Let $X$ be a message string, and $k$ be a key string, SipHash guarantees that even if an attacker have seen $X$ and $\code{SipHash}(X, k)$, an attacker who does not know $k$ would not be able to find out $k$ nor construct $\code{SipHash}(Y, k)$ for any message $Y \neq X$ which they have not seen before. Further cryptanalysis and mathematical proof of the promise above have been done by Christoph et al. \cite{dobraunig-2014} and Xin et al. \cite{xin-2019}.

SipHash has several unique characteristics that make it a suitable candidate for hardware implementation, namely simplicity, simple middle states, and minimal overhead \cite{aumasson-bernstein-2012}. The algorithm iterates a simple SipRound function utilised multiple times; hence the hardware component and circuit can be reused at different clock cycles. Siphash's middle states consists of only four 64-bit variables $v_0 ... v_4$. From a hardware implementation perspective, it minimises Flip-Flop (FF) usages, easing the design implementation on the board. Most importantly, the generated MAC is concise and of designated length (64 bits), bringing minimal and fixed consumption of hardware units.

\section{Related Work}

Although FPGA has been used to build various network appliances, only very little public research considered using FPGA against DNS exfiltration attacks. Thomas et al. \cite{thomas-2011} proposed an FPGA-based system for detecting malicious DNS network traffic, which only extracts DNS query domains and match it by hash against a list of hard-coded domain names with the memory. This approach is limited by the lack of real-time adaptability of the filter list and the inability to mitigate DNS exfiltration and flood attacks on the fly. 

With regards to using FPGA to mitigate DNS flood attacks, most related research aims at mitigating DDoS attacks as a whole instead of DNS flood attacks in particular \cite{hoque-2017, nagy-2018, tokusashi-2016, thinh-2016}. FPGAs have been used to detect DDoS attacks in real-time \cite{hoque-2017} and mitigate such attacks with very low reaction time \cite{nagy-2018}. However, most research aimed at mitigating DDoS attacks using FPGA focused primarily on DNS Amplification attacks \cite{nagy-2018, thinh-2016, tokusashi-2016}, as it was more prevalent and could be used to target individuals or smaller networks with low protection facilities in place. With reference to Section \ref{section:background-dnsflood}, the Mirai botnet's successful DNS flood attack against major DNS providers proves the inability of traditional mitigation methods against these new types of Layer 7 DDoS attacks. 

\section{Project Motivation}

This project introduces an FPGA-based solution that tackles two major rising threats exploiting the DNS protocol, DNS exfiltration attacks and DNS flood attacks. We will be improving on Thomas et al. \cite{thomas-2011}'s FPGA DNS filter by bringing real-time reconfigurability to our FPGA-based system. Innovatively, we will be further providing instantaneous	mitigation against DNS exfiltration transactions via hardware-accelerated DNS spoofing. Meanwhile, legitimate DNS transactions can proceed safely with minimal overhead.

Moreover, since most FPGA logic can be reused, namely traffic filtering and packet parsing in the aforementioned system, we further develop a separate FPGA operation mode to act as a low-latency Layer 7 firewall mitigating DNS flood attacks.

As the system will be used in high-bandwidth and latency-sensitive environments, we deem it necessary to develop an application-specific hardware solution for this task. Given the analysis in Section \ref{section:background-fpga} of ASIC and FPGA network processors, we preferred FPGA in our project implementation due to the cost and time limitations of a single-person final-year project.


\chapter{Design}

In this chapter, we will be presenting both the internal system design of the solution and the overall network design that this system will be put into practice. We will be elaborating on the design thought process and several design choices and considerations we made for the computing hardware and software control system. Also, we will be discussing some example topologies to illustrate how the solution will work to secure the network.

\section{System Design}

\subsection{Hardware Design}
\label{section:design-system-design-hardware}

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{imgs/hardware-design.png}
  \caption{Hardware Architectural Design}
  \label{fig:hardware-design}
\end{figure}

Since we are designing a network processing circuit, it is crucial that we are aware of the hardware interface we are interacting with. With Ethernet-II PHY ASIC available on most development boards with RJ45 connectors, we assumed that our design would retrieve incoming bits from a certain type of Media-Independent Interface (MII) family\footnote{See Section \ref{section:implementation-hardware-implementation} for introduction to MII family and implementation details} \cite{}. Similarly, we will be outputting bits to an MII-type interface as well at the end of processing. Hence, we can derive an initial architecture of a core processing component receiving from an input circuit and sending to an output circuit, as shown in Figure \ref{fig:hardware-design}.

Retrospectively speaking, clock domains was not thought through carefully at the design stages and later became a pitfall that cost significant amount of effort on regression and fixing. It's worth considering each component's clock domain, and several clock domain crossing methods when designing the system. Generally, PHY ASIC chips operates at a lower clock frequency than the FPGA core, resulting in different clock domains within the system. The initial clock signal from the FPGA's clock path is designed to go through a clock divider that generates suitable clock signals for each of the components in various domains. Inevitably throughout the design, there would be signals crossing clock domains (i.e. from receiver to core module). If left unsynchronised, the destination domain would experience various meta-stability issues\footnote{See Section \ref{section:implementation-hardware-debugging} for more information on Clock Domain Crossing (CDC) and signal metastability in hardware}. Thus, we introduce two asynchronous First-In-First-Out (FIFO) buffers that connections receiver to core, core to sender respectively, as shown between the clock regions in Figure \ref{fig:hardware-design}.

Within the Receiver and Sender component shown on the left and right side of Figure \ref{fig:hardware-design}, we aim to design a sequential circuit that utilises a Finite State Machine (FSM)  that mainly comprised of three parsing / packing stages, Ethernet-II, IPv4 and UDP. The sender ideally shall have an extra Ethernet Cyclic Redundancy Check 32-bit (CRC32) \cite{ieee802.3ethernet-2018} generation module that appends a 32-bit checksum to the packet. As of the core processing module, we would split it into two sub-component, one handling DNS parsing, Layer 7 filtering, FPGA administration and security checks, while the other one working on constructing proper DNS / payload response and calculating sizes and checksums for higher layer protocols (IPv4, UDP and DNS), as demonstrated in the Core section of Figure \ref{fig:hardware-design}.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/hardware-signal.png}}
  \caption{Hardware Signal Design}
  \label{fig:hardware-signal-design}
\end{figure}

After designing the top-level components and their main functionalities, we will be sketching the signal schematic of the design so as to determine each component's incoming and outgoing ports, as laid out in Figure \ref{fig:hardware-signal-design} The common input for almost all synchronous sequential logic is the clock pin (\code{CLK}). As suggested previously in the section, components within different clock domains shall receive \code{CLK} with their corresponding frequencies. The only exception here being the cross-domain asynchronous FIFOs which shall be activated by both \code{CLK} signals, one for writing to the FIFO queue and the other one for reading.

To avoid actively reading and writing data when not required, we utilised various Data Valid (DV), Enable (EN) and ACKnowledgement (ACK) signals in our components, as seen in Figure \ref{fig:hardware-signal-design}. Each sequential component in hardware is designed to constantly check against a set of signals when in idle state. The set of signals shall be specified by the hardware designer as the sensitivity list of a \proglang{VHDL}'s process, or a \proglang{Verilog}'s always-block. When the activation signal reaches Voltage Common Collector (VCC) level upon a rising clock edge, the sequential component shall transit into a working state and start reading and processing the bits from other incoming signals. For example, in the core input (\code{corein}) module (Figure \ref{fig:hardware-signal-design}), we utilise two sensitive signals, \code{CLK} and \code{RCV\_DV} to ensure the data processing starts when both are at VCC level. The receiving data will then be read and further processed at later rising clocked edges.

Besides clocking and activation signals, we also need to design data transfer ports across signals. We define a custom data struct (\code{D\_STRUCT}) to maintain all signals we need for processing and discards the rest unneeded packet information. The \code{D\_STRUCT} is an internal representation of the packet with relevant metadata and parsed DNS payload. This design allows us to keep a fixed length representation of variable length packets within the hardware system\footnote{See Section \ref{section:implementation-hardware-debugging} for issues with variable length descriptors in hardware}. Moreover, we made very effort to keep the \code{D\_STRUCT} nimble so as to avoid using unnecessary FIFOs delaying data transfers or wasting FPGA resources. Given the trade-off being the lack of support for payload longer than a pre-determined limit (128 bytes in our implementation\footnote{See Section \ref{section:implementation-hardware-implementation} for our detailed implementation of packet segmentation and the 128-byte limit}), we think it's worthwhile to proceed with this appoarch as we focus primarily on processing typically shorter DNS packets with minimal latency.

\subsection{Software Design}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/software-design.png}}
  \caption{Software Design}
  \label{fig:software-design}
\end{figure}

As part of our goal to develop a resilient system, we also designed an administrative software to control, monitor and reconfigure the FPGA in real-time. As with most network configuration software, we provide a Command Line Interface (CLI) front end to administrators. The top layers of Figure \ref{fig:software-design} the software front-end is comprised of a argument parser that understands launch options, a CLI parser that splits commands into function calls and their corresponding argument string, and several supporting features, namely CLI history and CLI auto-complete functionalities.

The software is designed to provide three main command families, \code{stats}, \code{test} and \code{admin}, corresponding to the three utilities in the middle layer of Figure \ref{fig:software-design}. 
\begin{itemize}
    \item \code{stats} commands provides user with the ability to understand the accumulated statistics and metrics on the FPGA filter, including packets received, DNS replies sent and etc.
    \item \code{test} commands enables user to test the hardware implementation of the FPGA system with integration tests and stress tests. This is to make sure the system on FPGA is functioning as expected and within the performance threshold.
    \item \code{admin} commands allows user to read and write DNS filter and operation mode configurations on the FPGA. It also provides convenience features for admins to save and load FPGA admin configurations as files on the controlling system.
\end{itemize}

Since certain commands such as \code{admin show curr} (which shows the currently editing admin configuration) do not need direct information exchange with the FPGA, we utilises a central controller logic to determine back-end operations and avoid unnecessary probes of the FPGA. Therefore, as shown in the bottom layers of Figure \ref{fig:software-design}, we provide various packaged functions that interfaces with the FPGA for the controller to interact with. These functions are mainly divided into senders and receivers, which relies two different raw sockets.

The receiver socket operates like a daemon thread that is listening to all network traffic on the FPGA interface. Noteworthy replies and metrics from the FPGA will be highlighted and saved to cache automatically upon capture. Then, relevant parsers or checkers will be called to verify or understand FPGA messages accordingly. 

The sender socket, on the other side of the spectrum, is only called when necessary. When an command is issued by the controller logic, the corresponding packet builder would build an administrative, test or probe packet that will be sent to the FPGA. A response might later be received and then coordinated by the receiver as discussed above.

With the aim to provide a future proof system, we also designed APIs for more complex algorithms such as entropy-based anomaly detectors to fetch and analyse the metrics captured by the FPGA. After the analysis, the algorithms can proactively re-configure the FPGA with several simple function calls. The APIs are designed as an extension, enabling developers to engineer complex algorithms making smarter decisions and push operational changes to FPGA instantaneously.

\section{System Usage Sample Topology}

\subsection{DNS Data Exfiltration}

The main innovative application of the FPGA-based system is to detect and mitigate DNS exfiltration attacks in real-time. Typically, the system would be deployed in a secured enterprise network with confidential information on its hosts (i.e. a cloud-based POS network, a military network with restricted Internet access). Assuming the network is originally setup with a firewall comprised of ASIC-based Layer 3 firewall and software-based Layer 7 firewall, bridging the connection between Wide Area Network (WAN) and the internal network switches. The FPGA shall connect to the network in a man-on-the-side manner with its operation mode set to "Man-on-the-Side DNS spoofing".  In this particular configuration, as an example, the FPGA checkers are set to:

\begin{itemize}
    \item MAC checkers: Disabled
    \item IP checkers: Check against a known list of internal DNS server IPs
    \item UDP Port checkers: Check for port 53
    \item DNS checkers: Check for \url{enterprise.local} and \url{essential.service}
\end{itemize}

If a DNS transaction satisfies all checker requirements, the packet would then be deemed legitimate and passed on to its intended recipients. If not, the FPGA would send out a Spoofed \code{NXDOMAIN} reply (or any administrator configured DNS \code{RCODE} reply) to the host.

We can further configure the ASIC-based Layer 3 firewall, or use a Network Test Access Point (TAP) device with the FPGA filter to archive the following:

\begin{itemize}
    \item When sending a DNS packet to WAN:
        \SubItem    {If it is not sent from the FPGA, redirect it to the FPGA}
        \SubItem    {If it is sent from the FPGA, send to WAN}
    \item When receiving a DNS packet from WAN
        \SubItem    {Redirect it to the FPGA}
\end{itemize}


\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/man-on-the-side-malicious-dns-query.png}}
  \caption{Man-on-the-Side FPGA sample topology - Sending malicious DNS exfiltration / tunnelling query}
  \label{fig:man-on-the-side-FPGA-send-malicious}
\end{figure}

Generally, the initiative of data exfiltration usually starts from a compromised host within the network. In the event where the host sends a malicious DNS query exfiltrating data / establishing a tunnel in Figure \ref{fig:man-on-the-side-FPGA-send-malicious}, the firewall would first redirect the query to FPGA. The FPGA would spot this DNS query suspicious and send a spoofed DNS reply back to the compromised host stopping the data exfiltration. 

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/man-on-the-side-valid-dns-query.png}}
  \caption{Man-on-the-Side FPGA sample topology - Sending legitimate DNS query}
  \label{fig:man-on-the-side-FPGA-send-legit}
\end{figure}

Legitimate DNS queries would not be affected within the network, as shown in Figure \ref{fig:man-on-the-side-FPGA-send-legit}. The packet will first be redirected to the FPGA for security checks. Once the FPGA deems the packet legitimate, it will forward the packet to the firewall, which will dispatch the packet to the upstream network. 

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/man-on-the-side-malicious-dns-reply.png}}
  \caption{Man-on-the-Side FPGA sample topology - Receiving malicious DNS exfiltration / tunnelling reply}
  \label{fig:man-on-the-side-FPGA-recv-malicious}
\end{figure}

In the extremely rare case where a malicious DNS exfiltration/tunnelling query did get sent to the attacker's C\&C due unknown issues, the reply can still be blocked by the FPGA if configured properly. In the case where the networks receives a DNS remote control reply, the firewall would divert the packet to FPGA for examination. The checker in the FPGA would fail and this would also trigger a spoofed DNS reply sent to the compromised host, as shown in Figure \ref{fig:man-on-the-side-FPGA-recv-malicious}.

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/man-on-the-side-valid-dns-reply.png}}
  \caption{Man-on-the-Side FPGA sample topology - Receiving legitimate DNS reply}
\end{figure}

As expected, legitimate DNS responses would remain unaffected throughout the process except for a minimal overhead added by the FPGA scrutinising the packet. The reply will be received by the firewall, forwarded to the FPGA. The FPGA will inspect the packet rapidly and deliver the verified response to the intended host untouched.

\subsection{DNS Flood}

This FPGA-based system can also be used to counter DNS flood attack on DNS name servers. As specified in Figure \ref{fig:dns-flood-man-in-the-middle}, we deploy the system in a man-in-the-middle manner in front of the DNS name servers. The FPGA operation mode is set to "Man-in-the-middle Filter", which would forward the exact payload upon passing the filter. With regards to the filters, we can configure them as:

\begin{itemize}
    \item MAC filters: Allow trusted Ethernet MACs from upstream ISP providers
    \item IP filters: Block a list of known IP address sending malicious IP addresses
    \item UDP Port filters: Allow only DNS port 53
    \item DNS filters: Allow only white-listed domain names and blocks all unknown domain names
\end{itemize}

Notably, we are placing the FPGA system as a hardware-based simple Layer 7 firewall in front of the system's software-based firewall. Utilising the unparalleled packet processing speed of FPGAs, the majority of flood traffic can be examined, logged and discarded by the FPGA with minimal latency before reaching the software firewall. Moreover, FPGAs can be scaled up linearly without extra overhead to accept and filter more flood traffic by simply connecting more FPGAs in parallel between the WAN and the software firewall. Therefore, the software firewall would enjoy much lower network load when under attack, as a large percentage of the junk traffic has already been blocked by the FPGAs. Thus, more complex checkups can be performed by the software firewall with its spare CPU time. With this setup, we can make sure the DNS name servers are well-protected with minimal performance and latency impact on the overall system.

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{imgs/man-in-the-middle-topology.png}}
  \caption{Man-in-the-Middle DNS Name Server FPGA sample topology - FPGA in the middle}
  \label{fig:dns-flood-man-in-the-middle}
\end{figure}

When it comes to certain  networks that serves not only DNS queries, the aforementioned topology in Figure \ref{fig:dns-flood-man-in-the-middle} might not be the best solution since all traffic except DNS are discarded by the FPGA. Instead, we can resort to the sample topology shown in Figure \ref{fig:dns-flood-man-in-the-middle-asic}. In this approach, we put FPGA on the side of the network only monitoring DNS transactions. Technically, this approach shall be no different as FPGA is still in the middle of all DNS traffic. The incoming traffic from WAN would ideally pass through an ASIC-based Layer 3 firewall which diverts all DNS traffic to the FPGA, and keeps other traffic on the normal route towards a Layer 7 firewall or directly towards a switch (in a less secure environment). Similar to the previous configuration, the FPGA would detect, log and drop packets not meeting the filter requirements. Legitimate payload is expected to pass the filter and get forwarded directly to the switch.

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{imgs/man-in-the-middle-amp-topology.png}}
  \caption{Man-in-the-Middle DNS Name Server FPGA sample topology - FPGA on the side}
  \label{fig:dns-flood-man-in-the-middle-asic}
\end{figure}

\chapter{Implementation}

In this chapter, we will be discussing implementation details of the FPGA-based system, including hardware engineering and debugging processes, as well as software development and integration steps.

\section{Hardware Implementation}
\label{section:implementation-hardware-implementation}

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[angle = 90, width=0.8\textwidth]{imgs/on-chip-impl.png}}
  \caption{Final Chip Implementation of the system on Artix-A7 (XC7A35TICSG324-1L)}
  \label{fig:on-chip-impl}
\end{figure}

\subsection{Finite State Machine (FSM) Implementation}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/FSM-impl.png}}
  \caption{Final State Machine Hardware Implementation}
  \label{fig:fsm-impl}
\end{figure}

FSM is one of the most commonly used design technique in stateful logic hardware circuits. More specifically, a hardware implementation of a $n$-state FSM requires three main logic blocks, as shown in Figure \ref{fig:fsm-impl}:
\begin{itemize}
    \item Next State Logic : a block of combinational circuit to determine the next state.
    \item State FFs and Current State Logic : a set of registers storing state information and a sequential logic circuit operating input signals based on the state. The number of FFs used is dependent on the encoding method, varying from $\ceil*{\log_2 n}$(binary encoded) - $n$(one-hot encoded).
    \item Output Logic : another stateless combinational circuit to output processed signals.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/binary-encoding-fsm.png}
    \end{minipage}%
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/grey-encoding-fsm.png}
    \end{minipage}
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/one-hot-fsm.png}
    \end{minipage}
    \caption{Binary-Encoded, Grey-Encoded and One-Hot-Encoded FSM}
    \label{fig:binary-grey-one-hot-encoded-FSM}
\end{figure}

With regards to the number of FFs in the state register, there are three main encoding methods shown in Figure \ref{fig:binary-grey-one-hot-encoded-FSM}, namely binary encoding, grey encoding and one-hot encoding.

Binary encoding is the most intuitive one to use when assigning numerical value to sequential states, and we are using as few bits as possible to encode the states. For an $n$-state FSM, binary encoding them sequentially shall yield $\ceil*{\log_2 n}$ bits. With the $4$-state FSM shown on the left side of Figure \ref{fig:binary-grey-one-hot-encoded-FSM}, $\ceil*{\log_2 4} = 2$ FFs would be needed. Binary encoding uses the least amount of memory, but the transitions can be very strict with timing\footnote{See Section \ref{section:implementation-hardware-debugging} for more information on hardware timing constraints \label{foot:timing-constraints}} in cases like going from $\mathtt{0x7f}$ to $\mathtt{0x80}$, with 8 bits to drive within one clock cycle (7 bits to set 0 and 1 bits to set 1).

Grey encoding consists of a sequence where only one bit shall change between the current value and next. This encoding developed aiming to minimise the bit flips when transitioning states so as to further relief the timing constraints and reduce dynamic power consumption\footref{foot:timing-constraints}. Notice the difference of encoding at State $S2$ and $S3$ in between the left FSM and the middle FSM in Figure \ref{fig:binary-grey-one-hot-encoded-FSM}, the grey-encoded FSM encodes $S2$ as $\mathtt{0b11}$ instead of $\mathtt{0b10}$ to ensure $S1 (\mathtt{0b01})\xrightarrow{}S2(\mathtt{0b11})$ flips only one bit, and similarly for $S2 (\mathtt{0b11})\xrightarrow{}S3(\mathtt{0b10})$.

Lastly, one-hot encoding is a scheme where each bit represents one single state. Thus, at any point in time, there shall only be one VCC (1) in states, while all other bits shall be Ground (GND) (0). With reference to the $4$-state FSM on the right side of Figure \ref{fig:binary-grey-one-hot-encoded-FSM}, the four states are encoded as $\mathtt{0b0001}$, $\mathtt{0b0010}$,  $\mathtt{0b0100}$ and $\mathtt{0b1000}$. This encoding method seems inefficient as encoding a $n$-state FSM requires $n$ FFs. However, one-hot encoding particularly useful in strict timing scenarios and simpflies the stimulus logic\footref{foot:timing-constraints}. The bits represents the states directly, and there shall be no state-decoding delay.

\begin{lstlisting}[language=VHDL, caption=One-Hot Encoded FSM Minimal Example in \proglang{VHDL}, label={lst:one-hot-fsm-minimal}]
TYPE state_t IS (IDLE, START, RUN, DONE);

ATTRIBUTE enum_encoding : STRING;
ATTRIBUTE enum_encoding OF state_t : TYPE IS "one-hot";

SIGNAL s, sin : state_t;

stim : PROCESS (clk)
BEGIN
  IF rising_edge(clk) THEN
    CASE s IS
      WHEN IDLE =>
        sin <= START;
      WHEN START =>
        sin <= RUN;
      WHEN RUN =>
        sin <= DONE;
      WHEN DONE =>
        sin <= IDLE;
    END CASE;
  END IF;
END PROCESS;

reg : PROCESS (clk)
BEGIN
  IF falling_edge(clk) THEN
    s <= sin;
  END IF;
END PROCESS;
\end{lstlisting}

Typically, we can provide behavioural description of the FSM states in \proglang{VHDL} and the synthesizer would be able to decide the encoding based on the complexity of the algorithm and the number of states. However, in certain cases where timing constraints are strict, we will have to specify encoding methods precisely via providing a signal description of the state registers.

Listing \ref{lst:one-hot-fsm-minimal} is a minimal example of One-Hot Encoded FSM in \proglang{VHDL}. Note that at line $4$ we set the attribute specifically to \code{one-hot}, and the synthesizer would generate One-Hot-Encoded FSM netlist accordingly. It is also worth noting that we store state \code{s} changes in signal \code{sin} on a rising clock edge and refresh the current state \code{s} on a falling edge. This is an attempt to make sure signal \code{s} would not run into metastability in a clock cycle when it is instantly used after modification\footnote{See Section 4.2 for more information on signal metastability in hardware}.

FSM is implemented in most components within the FPGA-based system, as parsing, processing and constructing network packets are all sequential logic and fits nicely with the stateful design of FSM.

\subsection{Top Level Module and Clock Divider}
\label{section:implementation-hardware-implementation-top-level-clock-divdier}

Figure \ref{fig:top-level-design} shows the implemented top-level design of the FPGA. As a streamlined packet processing system, the components are connected linearly as the packet flows from the incoming PHY receiver to the outgoing PHY transmitter.

\begin{landscape}
\begin{figure}[h!]
  \centering
  \includegraphics*[viewport={0 0 950 600}, height=\textwidth, width=2\textheight, keepaspectratio]{imgs/top-level-module.png}
  \caption{Top Level Module Overview - 1}
  \label{fig:top-level-design}
\end{figure}
\begin{figure}[h!]
  \ContinuedFloat
  \centering
  \includegraphics*[viewport={950 0 1900 600}, height=\textwidth, width=2\textheight, keepaspectratio]{imgs/top-level-module.png}
  \caption{Top Level Module Overview - 2}
  \label{fig:top-level-design}
\end{figure}
\begin{figure}[h!]
  \ContinuedFloat
  \centering
  \includegraphics*[viewport={1900 0 2850 600}, height=\textwidth, width=2\textheight, keepaspectratio]{imgs/top-level-module.png}
  \caption{Top Level Module Overview - 3}
  \label{fig:top-level-design}
\end{figure}
\end{landscape}

\begin{lstlisting}[language=VHDL, caption=Snippet of Clock Divider \code{clock.vhd}, label={lst:clock-vhd}]
ARCHITECTURE rtl OF clock IS
  SIGNAL clk_divider : unsigned(1 DOWNTO 0) := (OTHERS => '0');
BEGIN

  div : PROCESS (i_clk)
  BEGIN
    IF (rising_edge(i_clk)) THEN
      clk_divider <= clk_divider + 1;
    END IF;
  END PROCESS div;

  o25_clk <= clk_divider(1);
  o50_clk <= clk_divider(0);
 
END rtl;
\end{lstlisting}

As seen at the bottom of left of Figure \ref{fig:top-level-design}, \code{inst\_clock} is the clock divider component of the system. With reference to the Digilent Arty A7-35T manual\cite{digilent-arty}, a single crystal oscillator of $100$ MHz is connected to the board via pin E3. Considering system stability and overall design difficulties, we set the Artix-7 FPGA core to the default $50$ MHz Speedgrade 1 as per Xilinx clocking documentation \cite{xilinx-7-clocking}. Moreover, the interfacing PHY chip, Texas Instruments (TI) DP83848J, operates under a $25$ MHz clock when using MII ($50$ MHz when using Reduce MII (RMII)), which needs to be generated by the circuit and send to pin X1\cite{texas-instruments-dp83848x}. Therefore, we would need to introduce a oscillator signal processor to generate the clock we want in different domains of the system. As laid out in Listing \ref{lst:clock-vhd}, The clock divider operates upon the rising edge of the oscillator and produces slower clock using an internal unsigned rollover counter. Note that the clock divider is implemented as simple as possible without overflow checks to minimise the processing skew added to the FPGA fast clock path.

\subsection{PHY MII-family Interface, Receiver and Sender Peripheral}
\label{section:implementation-hardware-implementation-phy-mac-peripheral}

As described earlier, Arty-A7 35T has an on-chip Ethernet PHY ASIC peripheral (TI DP83848J), which operates under two interfaces, two Cat 5 speed grades and two duplex modes: RMII / MII, $10$ / $100$ Mbps and half/full duplex respectively \cite{texas-instruments-dp83848x}. All three settings can be configured via functional pins \code{LED\_LINK}, \code{LED\_SPEED} and \code{MII\_MODE}. In this project, we configure the PHY chip to operate with MII interface, $100$ Mbps full-duplex PHY link.

The MII-family interface is standard by IEEE 802.3u \cite{ieee802.3ethernet-2018} as the interface to connect a Ethernet Media Access Control (MAC) block to a PHY ASIC. It is considered to be an universal abstraction interface above various physical network medias (i.e. twisted pair, fiber optic and etc.). With the MII-family interface, any MAC hardware/software can connect to any PHY independent of the network signal transmission media.

The main receiving and transmitting signals on the interface includes:
\begin{itemize}
    \item \code{CRS}: Carrier Sense
    \item \code{COL}: Collision, which would not be used in full-duplex connections
    \item \code{RX\_CLK}, \code{TX\_CLK}: Receive / Transmit Clock
    \item \code{RXD[0-3]}: Receive Data [0-3],  (4 bits per clock cycle)
    \item \code{TXD[0-3]}: Transmit Data [0-3],  (4 bits per clock cycle)
    \item \code{RX\_DV}: Receive Data Valid (GND on this signal means data received on \code{RXD[0-3]} are just noise on the PHY)
    \item \code{TX\_EN}: Transmit enable (VCC on this signal means data provided on \code{TXD[0-3]} will be sent to the wire)
    \item \code{RX\_ER}, \code{TX\_ER}: Receive / Transmit Error (VCC on this signal means the PHY has received undecodable data / was unable to transmit data, possibly due to broken connections or unsuccessful negotiations)
\end{itemize}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{imgs/rcv-fsm.png}}
  \caption{\code{mac\_rcv.vhd} Module FSM Overview}
  \label{fig:rcv-fsm}
\end{figure}

Using the FSM sequential logic, we implemented our Receiver FSM as shown in Figure \ref{fig:rcv-fsm}. When \code{RX\_DV} is at VCC level, the receiver will start listening for Ethernet-II preambles at state \code{Preamble} and proceed with state transitions accordingly. During the parsing states, the receiver will analyse the packet as nibbles (4 bits) comes in from \code{RXD}, forms a \code{rcv\_data\_t} and checks for packet correctness at numerous checkpoints, including \code{ethertype}, \code{ipChecksum} and etc. As mentioned previously in design Section \ref{section:design-system-design-hardware}, the \code{dnsPkt} struct is limited to $1024$ bits (128 bytes) for the ease of signal transfer across components under strict timing requirements at the \code{Notify} state. Therefore, all bytes above this limit will be cut off and discarded beyond the receiver. Once the all input verification passes and the sturct \code{rcv\_data\_t} is properly constructed, the component would notify its predecessor by setting \code{EL\_DV} to high and return to the state \code{Preamble(Idle)}, ready for the next packet.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{imgs/snd-fsm.png}}
  \caption{\code{mac\_snd.vhd} Module FSM Overview}
  \label{fig:snd-fsm}
\end{figure}

Similarly, we also implemented the Sender interfacing with the PHY transmitter with a FSM shown in Figure \ref{fig:snd-fsm}. While the internal logic is in reverse order, sending signals to the interfaces rather than receiving, the general states of the FSM are alike. Note that the Sender operates based on a sending struct named \code{snd\_data\_t} with checksums and TTLs calculated and metadata imported from previous steps at state \code{Read}. As the sender pulls up \code{TX\_EN} and sends nibble streams to the \code{TXD} interface, the CRC32 calculator would also receive the stream and calculates the next-state FSM within the clock cycle. When all DNS payload has been transmitted, the 32-bit (8 byte) CRC sequence will be emitted by the calculator component and appended to the packet. Lastly but easily forgotten, Ethernet-II requires a interframe gap of 12 octets (24 nibbles) of \code{TX\_EN} at GND before a new packet can be transmitted. Not following the procedure properly would cause the next packet to be discarded by the intended recipients.

\subsection{Clock-Domain-Crossing FIFO}

One of the most common methods of transferring data across clock domains is to use a FIFO buffer. In hardware implementation, a CDC FIFO is usually a dual port RAM that operates separately with a read CLK and a write CLK. In our minimalistic CDC FIFO design shown in Listing \ref{lst:fifo-rcv-vhd}, we introduce two pairs of behavioural circuit corresponding to reading and writing to the FIFO. Each pair consists of a Next-State-Logic (NSL) which detects and operate read and write on the FIFO, and a register (REG) process that updates the index pointer state of the FIFO. Whenever there is packet left to be processed in the FIFO, the \code{buf\_not\_empty} signal would be pulled to VCC in order to let the predecessor aware of the new task at hand.

\begin{lstlisting}[language=VHDL, caption=Snippet of FIFO \code{FIFO\_receive.vhd}, label={lst:fifo-rcv-vhd}]
  fifo_wnsl : PROCESS (wclk)
  BEGIN
    IF (rising_edge(wclk)) THEN IF (w_en = '1') THEN
      ... ELSE ...
    END IF; END IF;
  END PROCESS;

  fifo_wreg : PROCESS (wclk)
  BEGIN
    IF falling_edge(wclk) THEN
      w_index <= win_index;
    END IF;
  END PROCESS;

  fifo_rnsl : PROCESS (rclk)
  BEGIN
    IF (rising_edge(rclk)) THEN IF (r_en = '1') THEN
        ... ELSE ...
    END IF; END IF;
  END PROCESS;

  fifo_rreg : PROCESS (rclk)
  BEGIN
    IF falling_edge(rclk) THEN
      r_index <= rin_index;
    END IF;
  END PROCESS;

  buf_not_empty <= '0' WHEN w_index = r_index ELSE '1';
\end{lstlisting}

In particular, we omit the commonly used Almost Full (AF) and Almost Empty (AE) flags in our FIFO as a implementation decision to discard the all 8 packets left unprocessed within the FIFO when index pointers collide, meaning the FIFO is full and unable to receive new packets. This is unlikely to happen as the core operates at much higher speed with 2 times lower processing time than the PHY clock domain. Hence, it is almost impossible to flood the core with too many packets to process. On the rare occasion it does happen, there would be no difference to discard new incoming packets or current packets in the FIFO as they are of equal importance. This also omits extra logic to check and monitor the FIFO status, which could yield minor latency improvements to the design.

\subsection{Core Input Processor}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/corein-fsm.png}}
  \caption{\code{i.vhd} Module FSM Overview}
  \label{fig:corein-fsm}
\end{figure}

The Core Input Processor is where the core processing and handling of network packets happens. Its high-level FSM is demonstrated in Figure \ref{fig:corein-fsm}. As the FIFO signals the \code{corein} unit, the full packet struct of \code{rcv\_data\_t} will be fetched at the \code{Read} state. The processor will then log the packet and identify the packet into three main categories:
\begin{itemize}
    \item Probe Packet : Packet that probes the FPGA for its current configuration and up-to-date metrics.
    \item Admin Packet : Packet sent from the FPGA administrators that carries new information for filters and operation mode configurations.
    \item Normal Network Packet : Packet that shall be checked and acted upon based on current on-board filter and operation configurations.
\end{itemize}

For a normal network packet received, it will be filtered / checked in the areas of MAC addresses, IP addresses, UDP ports, DNS headers and domain names. The filter will compare these information to given lists in the configuration. Dependent on the lists being blacklists or whitelists, the processor will provide a binary decision at the end of each procedure. By the end of all checks, the checkers will reach a final binary decision of pass or not pass. Then, based on the FPGA operation mode (man-in-the-middle filter mode, man-on-the-side spoof mode), the FPGA will take one of the four actions as instructed: forward the payload, forward the DNS transaction, spoof a DNS transaction, or discard the packet. Depending on the result, the core output module will be notified accordingly to deal with packet checksums and other relevant metadata. This is estimated to take at least 10 clock cycles (200 ns) under the best cases, and at most 120 clock cycles (2400 ns) with the largest payload and the most complex filters.  

Once a probe packet is logged, the FPGA will pack the current filter configurations and logger metrics by bits in a compact packet and pass on to the core output constructor for checksum calculations in less than 20 clock cycles (400 ns), relatively quick and straightforward.

Since an attacker can potentially disable the FPGA system via an admin packet, it requires extra authentication for the FPGA to recognise it as an legitimate administration update. The FPGA uses SipHash\footnote{See Section \ref{section:implementation-hardware-implementation-siphash} for more information about SipHash authentication} to validate the 64-bit MAC appended to the admin payload. Once it is verified, the FPGA will then update filter configurations and reset loggers and the system as instructed. Given the enhanced security on this section of the sequential circuit, it is expected to cost around 400 clock cycles (8000 ns) for a full filter update and a logger reset. This is acceptable even for latency sensitive use cases as updating filters could be rather rare and there is no reply actions needed afterwards.

If a packet needs to sent after the procedure, the processor shall pull up \code{snd\_en} and the Core Output Constructor will be notified to deal with the heavy lifting of checksums and size calculations.

\subsection{SipHash Authentication Component}
\label{section:implementation-hardware-implementation-siphash}

\begin{figure}[h!]
\centering
\begin{minipage}{.45\textwidth}
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/siphasher-fsm.png}}
  \caption{\code{siphasher.vhd} Module FSM Overview}
  \label{fig:siphasher-fsm}
\end{minipage}
\begin{minipage}{.45\textwidth}
  \makebox[\textwidth][c]{\includegraphics[width=0.6\textwidth]{imgs/coreout-fsm.png}}
  \caption{\code{o.vhd} Module FSM Overview}
  \label{fig:coreout-fsm}
\end{minipage}
\end{figure}

Given the complexity of SipHash calculations, we implemented a separate hardware component for its sole purpose. Across multiple variants of hash algorithms in the SipHash family, we adopted SipHash 2-4 due to its balance in security and computation complexity. With reference to the paper written by Jean-Philippe Aumasson \cite{aumasson-bernstein-2012}, SipHash 2-4 process is comprised of a key round, 2 compression rounds on every 64 bits of the payload, 2 compression rounds on the payload length and 4 finalising rounds. Each round involves the same set of obfuscation calculations named SipRound.

Given the steps, we therefore implemented the \code{siphasher} unit with the FSM shown in Figure \ref{fig:siphasher-fsm}. The 1024-bit (128-byte) payload of the admin packet is allocated with 960 bits (120 bytes) of payload and 64 bits (8 bytes) of MAC. Before the admin packet acknowledgement, the output of \code{siphasher} will be checked against the final 64 bits (8 bytes) in the payload to confirm the validity of the admin information. The SipHash key in hardware shall be edited in the \proglang{VHDL} module before synthesis and implementation and will be hardcoded into the FPGA core. This key shall be identical to the one used in the software control system, otherwise the hardware would deem the admin packet invalid and log the malicious behaviour of faking an administrator.

\subsection{Core Output Packet Constructor}

The final key component within the hardware implementation is the Core Output Packet Constructor \code{coreout}. This component is a sequential circuit that handles the struct transition from \code{rcv\_data\_t} to \code{snd\_data\_t}, while populating certain fields including sizes and checksums. The FSM overview containing construction steps can be viewed at Figure \ref{fig:coreout-fsm}. Current version of \code{coreout} would only compute IPv4 checksum and discard UDP checksums by default for latency reasons. The whole streamlined process shall take less than 10 clock cycles (200 ns) and the packet would be on its way to the sender FIFO queuing to be sent to the MII interface.


\section{Hardware Pitfalls \& Debugging}
\label{section:implementation-hardware-debugging}

This section describes all the pitfalls and challenges we met, investigated and eventually overcame during the project development process. This was a tirelessly grinding process with more than 600 hours spent. Hence, very likely this is going to be the longest section within the report. All code listings below are minimised real code snippets from HDL source files within the project. 

\subsection{HDL Pitfalls - Timing and Execution Models}

\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Faulty Signal Update in \proglang{VHDL}, label={lst:faulty-signal-update}]
SIGNAL s : STD_LOGIC;

proc : PROCESS (clk)
BEGIN
  IF (rising_edge(clk)) THEN
    s <= '1';
    IF (s = '1') THEN
    ... ELSE ...
    END IF;
  END IF;
END PROCESS;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Correct Signal Update in \proglang{VHDL}, label={lst:correct-signal-update}]
SIGNAL s, sin : STD_LOGIC;

proc : PROCESS (clk)
BEGIN
  IF (rising_edge(clk)) THEN
    sin <= '1';
    IF (s = '1') THEN
    ... ELSE ...
    END IF;
  END IF;
END PROCESS;

reg : PROCESS (clk)
BEGIN
  IF (falling_edge(clk)) THEN
    s <= sin;
  END IF;
END PROCESS;
\end{lstlisting}
\end{minipage}

At first glance, building hardware with HDL seems to be very similar to engineer software with a programming language, not to mention that \proglang{VHDL} has close resemblance to \proglang{Ada} while \proglang{Verilog} and \proglang{C} are pretty much alike. However, they are fundamentally different as HDL aims at describing static computing models with the explicit notion of concept "time", albeit concurrent or sequential, while software languages aims at producing list of instructions which executes in order. That is why HDLs resembles certain concurrent programming languages in their ability to model parallel processes that executes continuously independent of one another.

It is crucial to understand the timing model and promises of a HDL before we program a component. We initially implemented the faulty signal update logic as shown in Listing \ref{lst:faulty-signal-update}. From a software perspective, it would be perfectly valid to update \code{s} and reading it later on in a sequential statement. However, \code{VHDL}'s time model assumes that a signal happen would not happen immediately, but are scheduled at the end of the current process delta cycle. That is to say, this signal \code{s} will not be updated until all executing process have happened. This is a common pitfall when implementing hardware in \proglang{VHDL} and can be very hard to spot in complicated branching statements. Thus, as shown in Listing \ref{lst:correct-signal-update} and previous FSM Listing \ref{lst:one-hot-fsm-minimal}, we would design a current state signal \code{s} to read from and a next state signal \code{sin} to write to within a process, and explicitly updating \code{s} \textleftarrow \code{sin} after delay upon a falling clock edge. 

\begin{lstlisting}[language=VHDL, caption=Faulty Unrolled Loop Sample in \proglang{VHDL}, label={lst:unrolled-loop}]
proc : PROCESS (clk)
VARIABLE d : STD_LOGIC_VECTOR(255 DOWNTO 0) := (OTHERS => '0');
VARIABLE c : NATURAL RANGE 0 TO 255;
BEGIN
  IF (rising_edge(clk)) THEN
    FOR i IN 0 TO 255 LOOP
      IF (d(i) = '1') THEN
        c := c + 1;
      END IF;
    END LOOP;
    ... 
  END IF;
END PROCESS;
\end{lstlisting}

To allow direct usage after assignment within the same clock cycle / single activation, we can introduce variables in \proglang{VHDL} that would have a different timing promise according to the model. While signals are usually actual flip flops with persistent data stored, variables can be seen as temporary design objects that stores middle states. Hence, we made another mistake of abusing loops and variables usages in a single clock cycle, listed in Listing \ref{lst:unrolled-loop}. From a software perspective, it would be common to believe that the loop would execute one iteration per clock cycle. Instead, the synthesizer would unroll the loop completely and generate enough hardware to execute all iterations in parallel. Hence, we are effectively producing a circuit where the variable \code{c} gets incremented $0$-$255$ times at the same clock cycle within $20$ns. The implementation could only be possible when the data width of \code{d} is very small and the branches are extremely short. Otherwise, it would be impossible for the router to find an implementation within the timing constraint\footnote{See Section \ref{section:implementation-hardware-implementation-timing} for more information about timing constraints}.

\subsection{HDL Pitfalls - Simulation and Implementation Difference}
\label{section:implementation-hardware-debugging-hdl-pitfalls-simulation-and-implementation}

\begin{lstlisting}[language=VHDL, caption=Faulty \code{STD\_LOGIC\_VECTOR} comparison in \proglang{VHDL}, label={lst:faulty-comparison}]
IF (rd.dnsPkt(DOMAIN_LENGTH + OFFSET DOWNTO OFFSET)
  = f.dnsList(FILTER_INDEX)(DOMAIN_LENGTH DOWNTO 0)) THEN
  ...
END IF;
\end{lstlisting}

Besides the aforementioned pitfalls, HDLs themselves includes certain subsets within the language that is aimed at writing verification code (i.e. test benches) instead of circuit in hardware. One very subtle error we countered during the development can be attributed to this cause. When comparing domain names with DNS payload in hardware, we designed our component to follow the HDL shown in Listing \ref{lst:faulty-comparison}. The code simply compares a portion in the DNS payload from \code{OFFSET} TO \code{DOMAIN\_LENGTH + OFFSET}, with the No. \code{FILTER\_INDEX} string in the filter of the same width.

This piece of code works well within the \code{xsim} simulator and produced correct results when the domain names matches with an item in the filter list on behavioural simulation. However, when we later implement the design to hardware, none of the packets can be recognised and filtered out successfully. Yet the synthesizer, router and bitstream generator produces no warning nor error related to this component, which seems unreasonable. Upon further investigation and reading language specs, we realised that the synthesizer would not be able to produce the electric component for a variable width comparison in hardware. Instead, it would simply resort to the longest possible comparison that's viable for both sides. In this case, as the \code{dnsPkt} is longer than \code{f.dnsList(FILTER\_INDEX)}, it would compare the packet payload with the entire \code{f.dnsList}, regardless of the later specified range.

\begin{lstlisting}[language=VHDL, caption=Dynamic width \code{STD\_LOGIC\_VECTOR} comparison in \proglang{VHDL}, label={lst:correct-comparison}]
  IF (s.fpktc >= 32) THEN       -- 4 bytes
    IF (rd.dnsPkt((s.fpktsc + s.fpktc - 1) DOWNTO (s.fpktsc + s.fpktc - 32))
      = f.dnsList(s.fdnsc)((s.fpktc - 1) DOWNTO (s.fpktc - 32))) THEN
    ... ELSE ...
    END IF;
  ELSIF (s.fpktc >= 24) THEN    -- 3 bytes
    IF (rd.dnsPkt((s.fpktsc + s.fpktc - 1) DOWNTO (s.fpktsc + s.fpktc - 24))
      = f.dnsList(s.fdnsc)((s.fpktc - 1) DOWNTO (s.fpktc - 24))) THEN
    ... ELSE ...
    END IF;          
  ELSIF (s.fpktc >= 16) THEN    -- 2 bytes    
    IF (rd.dnsPkt((s.fpktsc + s.fpktc - 1) DOWNTO (s.fpktsc + s.fpktc - 16))
      = f.dnsList(s.fdnsc)((s.fpktc - 1) DOWNTO (s.fpktc - 16))) THEN
    ... ELSE ...
    END IF;                 
  ELSIF (s.fpktc >= 8) THEN     -- 1 byte
    IF (rd.dnsPkt((s.fpktsc + s.fpktc - 1) DOWNTO (s.fpktsc + s.fpktc - 8))
      = f.dnsList(s.fdnsc)((s.fpktc - 1) DOWNTO (s.fpktc - 8))) THEN
    ... ELSE ...
    END IF;
  ELSE
    ...
  END IF; 
\end{lstlisting}

Hence to work around this issue, we will have to eliminate all dynamic width comparison within the code. One alternative is to compare $n$ bytes at a time, until there is less than $n$ bytes left. At the later clock cycle, we would need to generate all $0 \;...\;n-1$ byte comparison to deal with the modulus. The minimised code structure can been seen at Listing \ref{lst:correct-comparison}. With that approach we would be able pass both behavioural simulation and post-implementation hardware simulation. However, as it finally turns out, this solution would impose too strict timing on the wire placement and substantial load on the driving signals to cause metastability and fail timing constraints\footnote{See Section \ref{section:implementation-hardware-implementation-metastability} and \ref{section:implementation-hardware-implementation-timing} for more information on signal metastability and timing constraints}. Further pipeling would be required to make the filter work properly in hardware.

\subsection{Timing Pitfalls - Clock Domain Crossing and Signal Metastability}
\label{section:implementation-hardware-implementation-metastability}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth, height=0.45\textwidth]{imgs/metastability-clock.png}}
  \caption{Metastable sample signal graph \cite{stephenson-2009}}
  \label{fig:metastability-clock}
\end{figure}

Metastability describes a phenomenon where an electronic element persists for an unbounded amount of time in an unstable (meta stable) state \cite{chaney-1973}. It causes digital systems to fail and exhibit indeterminate behaviour, which is troublesome to debug and fix. To explain why metastability happens, we would have to elaborate on how a signal transverse throughout a digital system. All components in FPGA have defined timing requirements that allow each register to receive input and produce output signals. To guarantee reliable operation, it is required to hold the input for a minimum register setup time $t_{SU}$ before the clock edge, and a minimum hold time after the clock edge $t_{H}$. The register output shall later be available after a clock-to-output delay $t_{CO}$. If either $t_{SU}$ or $t_{H}$ is violated, we will then receive output signals in so-called a meta stable state. To elaborate, the output signal could hover at any instant between VCC and GND for a random amount of time even after the previously guaranteed $t_{CO}$, as shown in Figure \ref{fig:metastability-clock}. At the end, the signal can either resolve to high or low, or even keep in the middle as an undefined signal. The signal transition through register is analogous to balancing a ball at the top of hill, as demonstrated in Figure \ref{fig:metastability-analogy}.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth, height=0.3\textwidth]{imgs/metastability-analogy.png}}
  \caption{Metastable signal analogy \cite{stephenson-2009}}
  \label{fig:metastability-analogy}
\end{figure}

As mentioned previously in Section \ref{section:implementation-hardware-implementation-top-level-clock-divdier}, the design features two clock domains, one for the PHY and MAC sender and receiver peripheral, and the other one for the FPGA core processor. As packet signals are crossing clock boundaries, the transition can happen at any point relative to the receiving clock edge of the capturing unit. Hence, the designer would be unable to predict the sequence of signals transmitted or the number of destination clock edges until the data finishes transmitting. As a result, the received values could very likely be incorrect and result in unexpected behaviour within the circuit. 
 
We were originally unaware of the CDC metastability issue and experienced unpredictable cases of packet losing and disappearing in the middle of the process. The issue was later relieved when we throttle the processing core down to $25$ MHz and use the same reference clock as the PHY peripherals. This is not ideal as we could have archived much 2 times faster speed with the processing.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=0.36\textwidth]{imgs/clock-skew.png}}
  \caption{Clock Skew demonstration \cite{arar-2018}}
  \label{fig:clock-skew}
\end{figure}

One might argue that the PHY reference clock is generated by the clock divider within the design, as specified in Section \ref{section:implementation-hardware-implementation-top-level-clock-divdier}; hence the design shall be synchronous and the timing requirements shall be predictable. However, this is unfortunately not the case. Although the clock division shall be fairly straightforward, it still takes a tiny portion of time and imposes a $\Delta t$ delay (skew) on the generated clocks, demonstrated in Figure \ref{fig:clock-skew}. The reference clock would then be provided to the PHY chip peripheral, while the design operates on the PHY feedback clock \code{E\_RX\_CLK} and \code{E\_TX\_CLK}. Thus, the PHY chip would also append an unknown amount of skew to the \code{E\_REF\_CLK}. In total, there shall be a 2-skew delay and 1:4 division from the original oscillator clock. Moreover, internal clocks goes through an isolated fast path named the clock path, which is hyper-sensitive to delays and skews. Thus, routing the significantly skewed PHY clock through the fast clock paths in FPGA is unfeasible, as it would cause substantial amount of timing errors across the core circuit. Consequently, it is a hardware-implementation decision to make the system an asynchronous design; hence making it vulnerable to CDC and metastability issues.
 
As explained in design Section \ref{section:design-system-design-hardware}: dual-clock FIFO (DCFIFO) is later designed and implemented in hardware for storing the middle signal states and guaranteeing timing requirements. Also, multiple handshaking logic signals including \code{DV}, \code{EN} and \code{ACK} are in place to further assure successful recognition of these CDC signals.

\subsection{Timing Pitfalls - Unintended Inferred Latches}

In this section, we will be referring to FFs exclusively as edge-triggered (synchronous) flip flops. Those level-triggered (asynchronous) storage units are referred to as latches for clarification.

In most FPGA designs, FFs are used commonly across most FPGA designs as the basic storage unit. Latches, on the other hand, are rare in FPGA designs but more common in ASICs suited in mainframes and supercomputers \cite{vasumathi-2018}. Latches deployed properly can improve machine cycle time in systems where the clock skew is a substantial fraction of the total clock cycle time \cite{vasumathi-2018}. However, in FPGAs, latch is a very poor match as its main benefit is almost non-existent in such a small chip where its clock distribution skew is effectively zero.

\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Inferred Latch with missing branch in \proglang{VHDL}, label={lst:inferred-latch-missing-branch}]
IF (R_EN = '1') THEN
  out_d <= in_d;
END IF;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Inferred Latch with incomplete assignment, label={lst:non-inferred-latch}]
-- default assignment 
out_d <= (OTHERS => '0');
IF (R_EN = '1') THEN
  out_d <= in_d;
END IF;

-- ELSE statement
IF (R_EN = '1') THEN
  out_d <= in_d;
ELSE
  out_d <= (OTHERS => '0');
END IF;
\end{lstlisting}
\end{minipage}

Latches are inferred in \proglang{VHDL} code primarily due to the language's guarantee when encountering unassigned signals. As shown in Listing \ref{lst:inferred-latch-missing-branch}, the \code{IF} statement sets \code{out\_d} to \code{in\_d} on branch, but doesn't specify what it shall be when not enabled. In this case, \proglang{VHDL} guarantees that the original value of \code{out\_d} would remain unchanged, hence a latch is inferred to preserve the original \code{out\_d} value. To avoid inferring a latch here, we shall append an \code{ELSE} statement before the \code{END IF} statement or specify a default assignment before the \code{IF} statement, as shown in Listing \ref{lst:non-inferred-latch}.

\begin{lstlisting}[language=VHDL, caption=An sample latch set on packet counter reaching $\mathtt{0x100}$, label={lst:sample-latch-issue}]
latch_en <= '1' WHEN packet_count = x"100" ELSE '0';
\end{lstlisting}

Given the asynchronous nature of latches, it can cause not only metastability issues as detailed in the previous Section \ref{section:implementation-hardware-implementation-metastability}, but also worse failures based on hardware implementation details. For example, we can infer a latch that is enabled every time upon packet counter reaching $\mathtt{0x100}$, regardless of the clock, with the code shown in Listing \ref{lst:sample-latch-issue}. This is seems fine as a purely combinatorial logic comprised of several gates and LUTs. However, the signal paths from \code{packet\_count} to \code{latch\_en} would never be perfect and there could be occassional glitches on it. For instance, imagine \code{packet\_count} is has not yet reached $\mathtt{0x100}$, but due to a signal path to \code{latch\_en} being marginally longer than the other route at an XOR-gate, \code{latch\_en} can go to VCC for an instant before settling down to GND, within its clock-to-output delay $t_{co}$. If it were an FF here, no issue will be caused as the FF would be clocked to wait for the next rising edge to update its internal value which is far away at this point in time. Unfortunately, given a latch here, the unintended value in will be set mistakenly. Making it worse, if the logic paths for the \code{latch\_en} signal was just routed differently (with different amount of delays), then the glitch behaviour would also change. It might work just fine in the design until something else completely unrelated changes and the fitter routes the whole design differently on the FPGA causing the failure to happen. This brings great uncertainty to the development and its corresponding fails would seem completely random. During the project, this happened in several occasions where weeks were spent just to figure out a single unneeded latch causing these types of problems.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=0.6\textwidth]{imgs/latch-timing.png}}
  \caption{Latch strict timing requirements \cite{jensen-2020}}
  \label{fig:latch-timing}
\end{figure}

Last but not least, using latches also imposes stricter timing requirements on the placer and router. As shown in Figure \ref{fig:latch-timing}, the left hand side of the Figure illustrates a signal traversing from a latch to a FF. Since the latch only guarantees stability during the lower half of the cycle, the logic depth is heavily restricted as we could waste the entire clock VCC period waiting for a latch to be stable. Instead, when traversing from a FF to another one, the router can utilise the full clock period as FFs would only change upon a rising clock edge.

In all, we have thoroughly analysed the timing pitfalls caused by implicitly inferring latches within the design. The main takeaway that we had throughout the debugging process is the in-depth understanding of why all sorts of issues happen with latches in the FPGA, and to avoid latches unless strictly necessary in all future designs.

\subsection{Timing Pitfalls - Pipelining and strict timing constraints}
\label{section:implementation-hardware-implementation-timing}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/timing-path.png}}
  \caption{Signal Propagation paths within a clock cycle from a source FF to a destination FF}
  \label{fig:timing-path}
\end{figure}

As hardware circuits execute, electric signals would be required to traverse through multiple logic gates within a clock cycle from a source register to a destination register. Upon traversing through each logic gate, there is a tiny portion of time taken for it to produce a stable output, called the gate delay. In a synchronous digital system, signals are regulated to move in lockstep, advancing one stage on each tick of the clock. This is usually enforced by synchronising end point registers (FFs), which forwards the input signal to output upon a rising edge. The number of gates throughout a specific path is called its logic depth. Due to technical limitations, most circuit designs cannot guarantee that all paths are created with the exact same logic depth, as demonstrated in Figure \ref{fig:timing-path}. This creates a very much undesired timing difference in the propagation delays across different paths, which is the major contributor to various glitches as a result of race conditions. In synchronous circuits specifically, the propagation time difference might also cause register timing violations in two different ways:

\begin{itemize}
    \item Maximum time violation: when the path logic is too deep that the signal would arrive too late on the receiving FF and misses the setup time of the register $t_{SU}$.
    \item Minimum time violation: when the path logic is not long enough so the signal changes too soon after the clock's state transition. The receive FF would not have enough hold time $t_{H}$ for the input signal to register properly.
\end{itemize}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/sta-in-action.png}}
  \caption{STA sample in \code{Vivado 2020.2} on a path with worst setup time of the MAC peripheral}
  \label{fig:sta-in-action}
\end{figure}

The exact of signal arrival can vary due to numerous (Power, Voltage, Temperature) PVT reasons. In an attempt to minimise the error caused in a complex system and ensure proper circuit operation, FPGA design tools like \code{Vivado} includes Static Timing Analysis (STA) as part of the place and route implementation process, shown in Figure \ref{fig:sta-in-action}.

Typically as the system gets more complex, there is higher chance that the design would trigger some maximum time violations. This is usually the result of having several logic paths being too deep within a clock cycle, or designing a signal that is required to drive too many other signals throughout the circuit. When implementing the DNS payload filter, as shown in Listing \ref{lst:correct-comparison}. Section \ref{section:implementation-hardware-debugging-hdl-pitfalls-simulation-and-implementation}, all the code within the listing describes the circuit behaviour within the same clock cycle. Essentially, we are attempting to put both packet size comparison (\code{s.fpktc} to bits count) and packet content comparison (\code{rd.dnsPkt} segment to \code{f.dnsList(s.fdnsc) segment}) logic circuits between two registers, which could be too depth for this board's timing requirements. Therefore, we need to pipeline the single task into various clock cycles to reduce the logic depth. As laid out in Listing \ref{lst:pipelined-design-dns-comparison}, multiple extra states are introduced to minimise the amount of logic within the same clock cycle. Although this is would surely increase the overall computation time and latency as it now takes four times more of clock cycles to process the same amount of information, we are strictly constrained by the overall chip capability with regards to timing and is unable to compress the logic even further.

Another example of timing constraint violation occurs when we abuse counter \code{c} in multiple occasions. Also, as seen in a minimal example from Listing \ref{lst:signal-driving-abuse}, we are using the same counter \code{c} in all cases across the parsing states. This would give router more difficulty when placing the registers storing \code{c} as the signals it drives are scatter around the chip. Signal traverse time is lower bounded by the speed of light and the distance would matter quite substantially on a picosecond level. As the designer, we would need to declare multiple signal counters (i.e. \code{r.ipc} and \code{r.udpc}) for different stages to make sure the placement is more localised and alleviate the drive load on each of the counters. 

Unlike software compilers which could make optimisations as long as the logic remains intact, the hardware synthesizer would not actively optimise the timing by allocating more registers or introduce more pipeline. Compared to software engineering, it is surely much more complex but we do gain much more complete control over the exact steps of computation when implementing a hardware based system.

\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Pipelined design from Listing \ref{lst:correct-comparison}, label={lst:pipelined-design-dns-comparison}]
WHEN CmpPktCheck =>
  ...
  IF (s.fpktc >= 32) THEN
    sin.s <= CmpPktArea32;
  ELSIF (s.fpktc >= 24) THEN
    sin.s <= CmpPktArea24;
  ELSIF (s.fpktc >= 16) THEN
    sin.s <= CmpPktArea16;
  ELSIF (s.fpktc >= 8) THEN
    sin.s <= CmpPktArea8;
  ELSE
    ...
  END IF;

WHEN CmpPktArea32 =>
  IF COMPARE_32_BITS THEN
  ... ELSE ...
  END IF;

WHEN CmpPktArea24 =>
  IF COMPARE_24_BITS THEN
  ... ELSE ...
  END IF;

WHEN CmpPktArea16 =>
  IF COMPARE_16_BITS THEN
  ... ELSE ...
  END IF;

WHEN CmpPktArea8 =>
  IF COMPARE_8_BITS THEN
  ... ELSE ...
  END IF;

WHEN CmpPktDone =>
  IF (s.fpktmf = '1') THEN
    -- there's a match
  ELSIF ... THEN
    -- all check done
  ELSE
    -- increment pkt counter
  END IF;

\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[language=VHDL, caption=Minimal Example in \proglang{VHDL} of a signal driving too many other signals, label={lst:signal-driving-abuse}]
TYPE rcv_t IS RECORD
s : state_t; -- Parse State
c : NATURAL RANGE 0 TO 65535; -- Count
END RECORD;
SIGNAL r, rin : rcv_t
:= rcv_t'(
  s => Preamble,
  c => 0,
);
BEGIN
  rcv_nsl : PROCESS (E_RX_CLK)
  BEGIN
  ...
    CASE r.s IS
      WHEN Preamble =>
        IF r.c = 15 THEN 
          rin.c <= r.c ...; ELSE ...;
        END IF;        
      WHEN StartOfFrame =>
        IF r.c = 1 THEN 
          rin.c <= r.c ...; ELSE ...;
        END IF;
      WHEN EtherMACDST =>
        IF r.c = 5 THEN 
          rin.c <= r.c ...; ELSE ...;
        END IF;
      WHEN EtherMACSRC =>
        IF r.c = 5 THEN 
          rin.c <= r.c ...; ELSE ...;
        END IF;
      WHEN EtherType =>
        IF r.c = 1 THEN 
          rin.c <= r.c ...; ELSE ...;
        END IF;
      WHEN IPIHL => ... -- Uses r.c
      WHEN IPVersion => ... -- Uses r.c
      WHEN IPDSCPECN => ... -- Uses r.c
      WHEN IPLength => ... -- Uses r.c
      WHEN IPID => ... -- Uses r.c
      ...
\end{lstlisting}
\end{minipage}

\subsection{Resource Pitfalls - FPGA Placement Constraint}

As mentioned in Section \ref{section:introduction-tools-hardware}, we initially started with the Spartan 3E Starter Kit board, which was released back in 2006. The board consists of a \code{XC3S500E} core with $10,476$ logic cells \cite{xilinx-documentation-2011-core} and a SMSC \code{LAN83C185} $10/100$ Mbps Ethernet PHY chip. However, once we have implemented the MAC peripherals on the PHY interface, as specified in Section \ref{section:implementation-hardware-implementation-phy-mac-peripheral}, the LUT usage is already around $9500$ ($90\%$). Moreover, as the utilisation is very high, the placer and router are having difficulties in making the design to meet the timing requirements due to space limitations. Hence, we would need to optimise the resource usage, otherwise the system would not be able to work properly, no to mention the to-be-designed core processing unit would simply be running out of logic slices. 

One key issue we have identified in the design is that we are using distributed RAM (LUTRAM) in lots of the places, instead of Block RAM (BRAM). Typically, if chunks of \code{STD\_LOGIC\_VECTOR} are declared in HDL with various types of access patterns, the synthesizer would infer a distributed RAM made up of LUT for you. Although distributed RAMs are more flexible and faster, they are made up of logic units within the FPGA. On the other hand, implementing BRAM requires the designer to follow a given synchronous data access pattern by addresses, with the same amount of bits retrieved in each clock cycle. Hence, the synthesizer could then realise the designer's intent and activate the FPGA's BRAM. BRAM is usually much larger but less flexible and can be slower than LUTRAM at times. 

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=0.56\textwidth]{imgs/utilisation-fpga.png}}
  \caption{Final Project Utilisation as reported by \code{Vivado 2020.2}}
  \label{fig:utilisation-fpga}
\end{figure}

However, as the parsing of a packet goes on, the packet array would get fragmented as different pieces of information would be required by separated components. As an experiment of using BRAMs has turned out, the access to BRAM would be heavily congested upon packet arrival as each component gets the signal to fetch fragments of the new packet. Hence, we would also need a FIFO at input and a dispatch unit at output to organise all BRAM accesses. This is considered to be inefficient as we are wasting more LUT to regulate BRAM access, and the amount used could be larger than simply using distributed RAM. Hence, we made the decision to purchase a newer board, the Digilent Arty-A735T with a \code{XC7A35T} FPGA boasting $33,280$ logic cells\cite{xilinx-documentation-artix}. As the final utilisation statistics turns out (Figure \ref{fig:utilisation-fpga}), the system would utilise $12,034 + 1704 = 13738$ logic units, surpassing the total amount available on the Spartan 3E Starter Kit.

\subsection{Hardware Debugging Methodologies}

As described in multiple sections above, debugging hardware is notoriously hard given its undeterministic results and limitations in tools. Before we deal with any debugging in hardware, it is crucial that we make sure simulation results behaves as expected, as it at least guarantees some level of correctness in the HDL code\footnote{See Section \ref{section:validation-simulation} for more information on HDL simulation}. 

In most cases, getting it right in simulator would not mean getting it to work in hardware. There could be various timing errors that cannot be reflected in the STA simulation which breaks the system. One of the most frequently used tools is the 4 LED lights on the board. As this represents 16 different states, we can put \code{LED <\null= x"6";} statements in the design to speculate the execution state. The placement of such statements needs to be placed deliberately as to make sure the LEDs would actually light up and hold instead of flashing quickly as the circuit executes on a nanosecond level.

One might ask, as a network project, we can utilise network packets to observe the internal states. This is partially true as we can retrieve certain information (i.e. packet missing, duplicate packets) from the network behaviour. However, most NIC nowadays would offload Ethernet, IP checksums and sizes calculations. In other words, the operating system network stack wouldn't even receive the packet if any of the payload is incoherent with checksums or size fields. In some cases, the FPGA would be faulty and keep sending packets at an 10us interval non-stop. These packets floods the testing machine, spikes the CPU usage and kills \code{Wireshark} as a result. Hence, it is feasible to debug the hardware during later stages of the development, but it is simply not possible to do so when we were still developing MACs and packet constructors.

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{imgs/ila.png}}
  \caption{ILA debugging trigger sample with \code{Vivado 2020.2}}
  \label{fig:ila-debug-fpga}
\end{figure}

Xilinx development environment provides a tool named ChipScope Integrate Logic Analyser (ILA) that can be synthesised into the hardware. It provides a IP core that reports to \code{Vivado} of the next several clock cycles (typically $1024$) of hardware internal states once triggered by a preset condition. In Figure \ref{fig:ila-debug-fpga}, we are showing an example of ILA debugging in progress. The bottom left corner shows that the ILA is triggered when \code{E\_TXD} reaches Preamble $\mathtt{0x5}$. Then, 1024 clock cycles worth of internal states on the \code{E\_TXD} signal is captured for further review. However, there are timing and resource limitations to ILA as in how much states to capture and how many signals can be captured at the same time. If violated, the system will exhibit unexpected behaviour and uploaded waveform from the FPGA will not be decodable.

\section{Software Control System}

The software system implementation can be mainly divided into three parts, namely the CLI interface (front-end), the FPGA utilities (middleware) and communication infrastructure with the FPGA (back-end).

\subsection{CLI interface}

The front-end functionalities are built based on the Edit line Library \code{libedit} that provides extensive CLI APIs. Hence, our application can have application history, auto-completion infrastructure out of the box.

\begin{lstlisting}[language=C++, caption=\code{COMMAND} structure,  label={lst:command-structure}]
typedef struct {
   const char *name;    /* User printable name of the function. */
   rl_icpfunc_t *func;  /* Function to call to do the job. */
   const char *doc;     /* Documentation for this function.  */
} COMMAND;

COMMAND commands[] = {
  { "admin", com_admin, "FPGA filter administrator utilities" },
  { "stats", com_stats, "FPGA filter stats utilities"},
  { "test", com_test, "FPGA test utilities"},
  ...
  { (char *)nullptr, (rl_icpfunc_t *)nullptr, (char *)nullptr }
};

COMMAND admin_commands[] = {
  { "show", com_admin_show, "show current / on-FPGA admin configurations [fpga/curr]" },
  { "edit", com_admin_edit, "edit current admin configurations" },
  ...
  { (char *)nullptr, (rl_icpfunc_t *)nullptr, (char *)nullptr }
};
\end{lstlisting}

When receiving an string input from the user, the parser will parser the string on space, and query the function pointer table named the \code{COMMAND} struct (shown in Listing \ref{lst:command-structure}), with regards to which function to be called. Designed with the delegation pattern in mind \cite{gamma-1995}, each higher-level command would query their own table and delegate the task to a lower-level command recursively  until a final utility function has been called. For example, when user types in \code{admin show fpga}, the command look up process will be delegated to \code{execute()}, \code{com\_admin()} and \code{com\_admin\_show()} respectively. Function \code{com\_admin\_show()} would receive the stripped down parameter \code{fpga} and call relevant middleware function to complete the task. 

\subsection{FPGA Utilities}

Functions in this layer handles the transition from FPGA messages to software understandable structures. In particular, it includes SipHash software implementation, bit compress and decompress functions and bit/byte-endianness correction utilities. There are also several temporary states / caches stored within this layer including the current admin configuration in edit, waiting to be applied, the last queried FPGA status and metrics and etc. Several notable functions in this layer are:
\begin{itemize}
    \item \code{parse\_probe\_pkt} that uses bit manipulation functions to parse the bit-compressed FPGA probe response and validate its integrity.
    \item \code{write\_admin\_pkt} that writes administration payload and compress into FPGA-recognisable bit format, and append SipHash verification MAC to the payload.
    \item \code{test\_dns\_filters}, \code{test\_reply\_filters}, ... that generates random packets and validates the hardware correctness.
    \item \code{print\_fpga\_stats}, \code{print\_fpga\_configurations}, ... that aligns and prints the messages to \code{stdout} for users to examine.
\end{itemize}

\subsection{FPGA Communication Infrastructure}

In the bottom layer, there are two main threads that handles sending and receiving packets respectively to and from the FPGA. Since sending packets is only a function call, we integrate this behaviour within the main thread. However, receiving reports and metrics from the FPGA require constant listening that may block the main thread. Hence, we utilised another thread to do the receiving asynchronously. When the thread receives a packet on the raw socket that belongs to the FPGA and is of interest, it would record the packet, call the parser and store the parsed structure in the cache.

\subsection{Other software tools and utilities}

Throughout the development process, we also developed several utilities that was particularly useful. 

\begin{itemize}
    \item \code{admin\_builder}: a command line tool that builds a desired administration packet based on command line arguments, saves and loads admin configurations to and from files. It was later integrated into the system as command \code{admin}.
    \item \code{crc32\_checker}: a command line tool that generates/checks Ethernet CRC32 FCS from binary / text files. It was particularly useful at early stages of debugging to compare its result with the hardware simulator.
    \item \code{siphash\_checker}: a command line tool that generates/checks SipHash MAC checksum from binary / text files. It was adapted based on the software reference implementation \cite{aumasson-bernstein-2012} to validate implementation in hardware.
    \item \code{pkt\_recevier/pkt\_sender}: a command line tool that interacts with raw socket (Linux) / BPF (BSD-based systems) to send and receive packets to and from the FPGA. It takes input from command line arguments and adapts to lower level OS accordingly. It was later integrated into the back-end of the software system.
\end{itemize}

\chapter{Validation and Evaluation}

In this section, we will be elaborating on our tests and validations in place to ensure the FPGA-based system's correctness. Also, we will be measuring the system's performance and evaluating its usability in large scale networks. 

\section{Simulation Testbenches}
\label{section:validation-simulation}

We first validate the HDL design's correctness via constructing test benches in \proglang{VHDL}. The testbench is comprised of several components listed below:

\begin{itemize}
    \item \code{test\_main.vhd}: The top-level test unit instantiates the \code{main} component and defines clock signals simulating the oscillators. Tests are called in the simulation main function of the unit.
    \item \code{test\_dns\_pkts.vhd}, \code{test\_srcip\_pkts.vhd} ... : These units include test cases for each hardware features, compiled by calling infrastructural packet functions in a given sequence. 
    \item \code{test\_pkt\_infra.vhd}: This unit contains infrastructural test functions simulating fragments of packets sent to the MII interface. Basic package functions such as \code{receive\_preamble()} and \code{receive\_admin\_config()} are provided for higher level test cases.
\end{itemize}

\begin{figure}[h!]
  \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{docs/report/imgs/simulation.png}}
  \caption{Behavioural Simulation of the system in \code{Vivado 2020.2}}
  \label{fig:behavioural-simulation-fpga}
\end{figure}

Within the behavioural simulation of test benches, HDL object states are transparent for review, similar to single-stepping with a software debugger. Breakpoints can be set in HDL code to monitor signal states at a given instant. This step confirms the general logic correctness independent of the actual implementation. An example of behavioural testbench simulation is shown in Figure \ref{fig:behavioural-simulation-fpga}. Zooming in on the waveform would enable us to view the anticipated HDL state at each instant on a picosecond level.

After synthesis and netlist generation, we can instruct \code{Vivado} to simulate on a netlist level, or even on a chip level after implementation. The simulation results can be more accurate and closer to hardware. However, we would not be able to set breakpoints or view the HDL objects anymore. Instead, we will be provided with hardware netlists information at each instant.

Our system has passed all test cases simulations at behavioural, netlist and chip levels, with a $100\%$ simulation coverage.

\section{Hardware Integration Testing via the Software System}

With reference to the aforementioned issues in Section \ref{section:implementation-hardware-debugging}, we constantly faced regressions in functionalities upon completely unrelated changes in HDL. Hence, we developed a hardware verification utility in software to conduct integration testing, so as to spot any function regression at the earliest notion. The system was later integrated into the software control system as the \code{test} command. 

When testing the hardware, the system would first generate pseudo-random admin configurations with MAC/IP addresses, UDP ports and DNS domains. It will then be applied to FPGA and instantly probed and read back to validate the deployment and probe functionality. Then, the system would stress test the FPGA via flooding packets with random patterns as fast as the software is capable of to validate the checkers in FPGA. The same procedure will be repeated with emphasis on different parts of the filters, operations modes and packet constructors, each with 3 runs, to make sure the correctness of the hardware circuit. In total, roughly 1200 packets will be communicated on the wire throughout the tests. 

SCREENSHOT OF TESTS in action.

As shown in Figure \ref{}, the submission candidate version of the system passes all integration and stress tests.

\section{Evaluation}

We further evaluate the FPGA-based design and system in terms of performance, power consumption and scalability.

The performance is evaluated by executing the aforementioned software stress tests with a Raspberry Pi 4i (Broadcom BCM2711 with Integrated Gigabit Ethernet Controller), and measuring the reply latency. Given the equipment limitations, we are unable to accurately measure the exact latency induced by the Raspberry Pi NIC and the signal travelling on the Ethernet cable. Hence we are estimating the extra latency according to publicly available data sheets and standards. According to IEEE 802.3 \cite{ieee802.3ethernet-2018}, the propagation delay on an standard Cat5 Ethernet cable would be around $5.30 \; ns/m$. Thus we calculate the round trip latency on the $2.5m$ cable as  
$$
t_r = 2.5\; m \times 2 \times 5.30\; ns /m = 0.0265 \; \mu s
$$ 
A gigabit NIC encoding and decoding a packet for the twisted pair Cat 5 cable is estimated to take around $5 \; \mu s$ \cite{texas-instruments-dp83848x}, including system bus and Interrupt Request (IRQ) latency of $5 \; \mu s$. Hence, we are estimating the total extra latency of one decode, one encode and one interrupt to be around $15.00\; \mu s$.

Wireshark records the packet communications throughout the test cases and we calculated the trimmed Round Trip Time (RTT) mean (95\%) as follows:
\begin{itemize}[noitemsep]
    \item Network Packets passing all checkers: $27.238\; \mu s$
    \item Spoofed DNS Reply Packets: $28.753\; \mu s$
    \item Probe Packets: $67.586\; \mu s$
\end{itemize}
See Table \ref{table:performance-test} for RTT measurements of each test case.

\begin{table}[h!]
\begin{tabular}{ll|ll|ll|ll}
\hline
testcase  & avg ($\mu s$) & testcase & avg ($\mu s$) & testcase   & avg ($\mu s$) & testcase  & avg ($\mu s$) \\ \hline
srcmac-b0 & 32.276   & srcip-b0 & 31.942   & srcport-b0 & 39.743   & dns-b0      & 24.766   \\
srcmac-b1 & 29.612   & srcip-b1 & 29.412   & srcport-b1 & 36.372   & dns-b1      & 33.065   \\
srcmac-b2 & 29.466   & srcip-b2 & 22.334   & srcport-b2 & 26.45    & dns-b2      & 27.856   \\
srcmac-w1 & 45.432   & srcip-w1 & 23.285   & srcport-w1 & 34.242   & dns-w1      & 18.767   \\
srcmac-w2 & 27.282   & srcip-w2 & 17.747   & srcport-w2 & 17.174   & dns-w2      & 18.876   \\
dstmac-b0 & 15.761   & dstip-b0 & 31.013   & dstport-b0 & 15.83    & spoof-reply & 28.753   \\
dstmac-b1 & 23.267   & dstip-b1 & 36.306   & dstport-b1 & 29.067   & probe       & 34.679   \\
dstmac-b2 & 20.942   & dstip-b2 & 16.31    & dstport-b2 & 30.15    &             &          \\
dstmac-w1 & 22.123   & dstip-w1 & 17.205   & dstport-w1 & 17.565   &             &          \\
dstmac-w2 & 18.568   & dstip-w2 & 52.721   & dstport-w2 & 40.405   &             &          \\ \hline
\end{tabular}
\caption{RTT measures of all test cases on Raspberry Pi 4B}
\label{table:performance-test}
\end{table}

Therefore, we estimate the processing latency of the FPGA as:
\begin{itemize}[noitemsep]
    \item Network Packets passing all checkers: $27.238\; \mu s - 15\mu s = 12.238\; \mu s$
    \item Spoofed DNS Reply Packets: $28.753\; \mu s - 15\; \mu s = 13.753\; \mu s$
    \item Probe Packets: $34.679\; \mu - 15\; \mu s = 19.679\; \mu s$
\end{itemize}
If we count in the PHY encoding / decoding time taken on the FPGA board, we estimate the processing latency of the FPGA design as 
\begin{itemize}[noitemsep]
    \item Network Packets passing all checkers: $12.238\; \mu s - 10\; \mu s= 2238\; ns$
    \item Spoofed DNS Reply Packets: $13.753\; \mu s - 10\; \mu s = 3753\; ns$
    \item Probe Packets: $19.679\; \mu - 10 \mu s = 9679\; ns$
\end{itemize}
The experiment results are largely coherent with the estimates we made during implementation in Section \ref{section:implementation-hardware-implementation}.

WRITE ABOUT POWER CONSUMPTION AND SCALABILITY IN PERFORMANCE AND POWER

\newpage


Power Consumption, Performance

\chapter{Conclusions and Future Work}

\section{Conclusions}

\section{Future Work}
How the project might be continued, but do not give the impression you ran out of time!

\addcontentsline{toc}{chapter}{References}
\printbibliography[title=References]

\appendix

\chapter{Project Demo}

\chapter{Code Listing}

URL \url{https://github.com/magetron/cpu-fpga-nwofle}

\chapter{Original Project Proposal}

\end{document}